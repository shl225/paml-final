{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "beda04ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import re\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "import joblib\n",
    "import warnings\n",
    "from collections import defaultdict\n",
    "\n",
    "#sklearn\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, PowerTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.feature_selection import SelectFromModel, RFE\n",
    "\n",
    "#metrics\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score, mean_absolute_error\n",
    "\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "seed = 42\n",
    "modelfolder = \"models\"\n",
    "os.makedirs(modelfolder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "901f18fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Data Loading\n",
      "=============================================\n",
      "Reading gamescsv...\n",
      "Size of games is: 65111\n",
      "Reading steamdbjson...\n",
      "Size of steamdb data is: 53981\n",
      "Checking overlap:\n",
      "Games data unique: 65109, steamdb data unique: 53981, overlap: 47609\n",
      "Overlap percent: 88.19584668679721%\n",
      "Merging...\n",
      "Checking duplicates in appid after merge\n",
      "Dropped 2 duplicate rows new shape is (71481, 59)\n",
      "Merged data has 71481 unique rows in total\n",
      "num columns is: 59\n",
      "Initial shape: (71481, 59)\n"
     ]
    }
   ],
   "source": [
    "print(\"1) Data Loading\")\n",
    "print(\"=============================================\")\n",
    "\n",
    "gamesdata = pd.DataFrame(columns=['app_id'])\n",
    "steamdata = pd.DataFrame(columns=['app_id'])\n",
    "mergeddata = pd.DataFrame()\n",
    "alldata = pd.DataFrame()\n",
    "\n",
    "try:\n",
    "    print(\"Reading gamescsv...\")\n",
    "    gamesdata = pd.read_csv('games.csv')\n",
    "    print(\"Size of games is: \" + str(len(gamesdata)))\n",
    "    if 'App ID' in gamesdata.columns:\n",
    "        gamesdata = gamesdata.rename(columns={'App ID': 'app_id'})\n",
    "    elif 'appid' in gamesdata.columns:\n",
    "        gamesdata = gamesdata.rename(columns={'appid': 'app_id'})\n",
    "    if 'app_id' in gamesdata.columns:\n",
    "        gamesdata['app_id'] = gamesdata['app_id'].astype(str)\n",
    "except FileNotFoundError:\n",
    "    print(\"couldnt find gamescsv\")\n",
    "\n",
    "try:\n",
    "    print(\"Reading steamdbjson...\")\n",
    "    with open('steamdb.json','r',encoding='utf-8') as f:\n",
    "        stdata = json.load(f)\n",
    "    steamdata = pd.DataFrame(stdata)\n",
    "    print(\"Size of steamdb data is: \" + str(len(steamdata)))\n",
    "    steamdata = steamdata.rename(columns={\n",
    "        'sid': 'app_id',\n",
    "        'name': 'title_steamdb',\n",
    "        'full_price': 'steamdb_full_price',\n",
    "        'current_price': 'steamdb_current_price'\n",
    "    })\n",
    "    if 'app_id' in steamdata.columns:\n",
    "        steamdata['app_id'] = steamdata['app_id'].astype(str)\n",
    "except FileNotFoundError:\n",
    "    print(\"steamdbjson not found\")\n",
    "except json.JSONDecodeError:\n",
    "    print(\"Steamdbjson may be corrupted\")\n",
    "\n",
    "if not gamesdata.empty and not steamdata.empty and 'app_id' in gamesdata.columns and 'app_id' in steamdata.columns:\n",
    "    print(\"Checking overlap:\")\n",
    "    gameids = set(gamesdata['app_id'])\n",
    "    steamids = set(steamdata['app_id'])\n",
    "    overlapids = gameids.intersection(steamids)\n",
    "    print(\"Games data unique: \" + str(len(gameids)) + \", steamdb data unique: \" + str(len(steamids)) + \", overlap: \" + str(len(overlapids)))\n",
    "    if min(len(gameids), len(steamids))>0:\n",
    "        print(\"Overlap percent: \" + str(len(overlapids)/min(len(gameids), len(steamids))*100) + \"%\")\n",
    "\n",
    "    if 'tags' in gamesdata.columns and 'tags' in steamdata.columns:\n",
    "        steamdata.rename(columns={'tags':'tags_steamdb'}, inplace=True)\n",
    "        print(\"renamed tags column to tags_steamdb so no collisions\")\n",
    "\n",
    "    print(\"Merging...\")\n",
    "    mergeddata = pd.merge(gamesdata, steamdata, on='app_id', how='outer', suffixes=('','_steamdb'))\n",
    "\n",
    "    print(\"Checking duplicates in appid after merge\")\n",
    "    initial_len = len(mergeddata)\n",
    "    mergeddata.drop_duplicates(subset='app_id', keep='first', inplace=True)\n",
    "    diff_len = initial_len - len(mergeddata)\n",
    "    if diff_len>0:\n",
    "        print(\"Dropped \" + str(diff_len) + \" duplicate rows new shape is \" + str(mergeddata.shape))\n",
    "\n",
    "    duplicatecols = mergeddata.columns[mergeddata.columns.duplicated()]\n",
    "    if len(duplicatecols)>0:\n",
    "        print(\"Found duplicate cols: \" + str(duplicatecols.tolist()))\n",
    "        for ccol in duplicatecols:\n",
    "            newc = ccol+\"_duplicate\"\n",
    "            print(\"Renaming \" + ccol + \" to \" + newc)\n",
    "            cpos = np.where(mergeddata.columns==ccol)[0]\n",
    "            mergeddata.columns.values[cpos[1]] = newc\n",
    "\n",
    "    mergeddata = mergeddata.reset_index(drop=True)\n",
    "    print(\"Merged data has \" + str(len(mergeddata)) + \" unique rows in total\")\n",
    "    print(\"num columns is: \" + str(mergeddata.shape[1]))\n",
    "else:\n",
    "    print(\"Can't merge: no data or columns missing\")\n",
    "    if not gamesdata.empty and 'app_id' in gamesdata.columns:\n",
    "        mergeddata = gamesdata\n",
    "        print(\"Games data only\")\n",
    "    else:\n",
    "        print(\"No data\")\n",
    "        mergeddata = pd.DataFrame()\n",
    "\n",
    "if not mergeddata.empty:\n",
    "    alldata = mergeddata.copy()\n",
    "    print(\"Initial shape: \" + str(alldata.shape))\n",
    "else:\n",
    "    print(\"Empty df being used\")\n",
    "    alldata = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3006231f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2) Data Preprocessing\n",
      "===============================================\n",
      "Found duplicates after cleaning: ['tags']\n",
      "Renamed cols so no collisions occur\n",
      "Fixing price columns\n",
      "Creating target_price if possible\n",
      "picking launch_price as main target\n",
      "these prices look too big dividing by 100 for anything above 200\n",
      "Handling outliers (capping)\n",
      "capping 104 outliers to range: 0.99 to 123.49000000000001\n",
      "made a log1p price col log_price\n",
      "made price buckets\n",
      "dropped 6372 rows no target or invalid price\n",
      "Preproc final shape is (65109, 62)\n"
     ]
    }
   ],
   "source": [
    "print(\"2) Data Preprocessing\")\n",
    "print(\"===============================================\")\n",
    "\n",
    "if not alldata.empty:\n",
    "    #cleaning col names\n",
    "    alldata.columns = [re.sub(r'\\W+','_',col).lower().strip('_') for col in alldata.columns]\n",
    "\n",
    "    if alldata.columns.duplicated().any():\n",
    "        dupc = alldata.columns[alldata.columns.duplicated()].unique()\n",
    "        print(\"Found duplicates after cleaning: \" + str(dupc.tolist()))\n",
    "        for cc in dupc:\n",
    "            ccpos = np.where(alldata.columns==cc)[0]\n",
    "            for i in range(1,len(ccpos)):\n",
    "                alldata.columns.values[ccpos[i]] = cc+\"_dup\"+str(i)\n",
    "        print(\"Renamed cols so no collisions occur\")\n",
    "\n",
    "    if 'app_id_steamdb' in alldata.columns and 'app_id' in alldata.columns:\n",
    "        alldata = alldata.drop(columns=['app_id_steamdb'])\n",
    "    if 'title_steamdb' in alldata.columns and 'title' in alldata.columns:\n",
    "        alldata['title'] = alldata['title'].fillna(alldata['title_steamdb'])\n",
    "        alldata = alldata.drop(columns=['title_steamdb'])\n",
    "\n",
    "    if 'title' not in alldata.columns:\n",
    "        alldata['title'] = 'Unknown Game'\n",
    "    alldata['title'] = alldata['title'].fillna('Unknown Game')\n",
    "\n",
    "    def extractprice(astr):\n",
    "        if pd.isna(astr):\n",
    "            return np.nan\n",
    "        try:\n",
    "            s = str(astr)\n",
    "            s = re.sub(r'[^\\d.,]','',s)\n",
    "        except:\n",
    "            return np.nan\n",
    "        if '.' not in s and ',' in s:\n",
    "            s = s.replace(',','.')\n",
    "        s = s.replace(',','')\n",
    "        mm = re.search(r'(\\d+(\\.\\d+)?)', s)\n",
    "        return float(mm.group(1)) if mm else np.nan\n",
    "\n",
    "    pcols = ['launch_price','steamdb_full_price','steamdb_current_price']\n",
    "    print(\"Fixing price columns\")\n",
    "    for c in pcols:\n",
    "        if c in alldata.columns:\n",
    "            alldata[c] = alldata[c].apply(extractprice)\n",
    "\n",
    "    print(\"Creating target_price if possible\")\n",
    "    used_target = False\n",
    "    if 'steamdb_full_price' in alldata.columns and alldata['steamdb_full_price'].notna().sum() > (alldata['launch_price'].notna().sum() if 'launch_price' in alldata.columns else 0):\n",
    "        alldata['target_price'] = alldata['steamdb_full_price']\n",
    "        if 'launch_price' in alldata.columns:\n",
    "            alldata['target_price'] = alldata['target_price'].fillna(alldata['launch_price'])\n",
    "        print(\"picking steamdb_full_price as main target then fallback on launch_price if missing\")\n",
    "        used_target = True\n",
    "    elif 'launch_price' in alldata.columns:\n",
    "        alldata['target_price'] = alldata['launch_price']\n",
    "        print(\"picking launch_price as main target\")\n",
    "        used_target = True\n",
    "    else:\n",
    "        print(\"no price col found for target\")\n",
    "        alldata['target_price'] = np.nan\n",
    "\n",
    "    if used_target and alldata['target_price'].notna().any():\n",
    "        medianp = alldata['target_price'].median()\n",
    "        maxp = alldata['target_price'].max()\n",
    "        if pd.notna(medianp) and pd.notna(maxp) and (medianp>100 or maxp>200):\n",
    "            print(\"these prices look too big dividing by 100 for anything above 200\")\n",
    "            for cc in pcols + ['target_price']:\n",
    "                if cc in alldata.columns:\n",
    "                    hmask = (alldata[cc]>200) & alldata[cc].notna()\n",
    "                    if hmask.any():\n",
    "                        alldata.loc[hmask,cc] = alldata.loc[hmask,cc]/100.0\n",
    "\n",
    "    print(\"Handling outliers (capping)\")\n",
    "    if 'target_price' in alldata.columns:\n",
    "        q1 = alldata['target_price'].quantile(0.01)\n",
    "        q3 = alldata['target_price'].quantile(0.99)\n",
    "        iqr = q3 - q1\n",
    "        lower = max(0.99, q1 - 1.5*iqr)\n",
    "        upper = q3 + 1.5*iqr\n",
    "        capmask = (alldata['target_price']<lower) | (alldata['target_price']>upper)\n",
    "        if capmask.any():\n",
    "            print(\"capping \" + str(capmask.sum()) + \" outliers to range: \" + str(lower) + \" to \" + str(upper))\n",
    "            alldata.loc[alldata['target_price']<lower,'target_price'] = lower\n",
    "            alldata.loc[alldata['target_price']>upper,'target_price'] = upper\n",
    "\n",
    "    if 'target_price' in alldata.columns:\n",
    "        alldata['log_price'] = np.log1p(alldata['target_price'])\n",
    "        print(\"made a log1p price col log_price\")\n",
    "\n",
    "        pricebreaks = [0,4.99,9.99,14.99,19.99,29.99,39.99,59.99,float('inf')]\n",
    "        plabels = ['$0-$4.99','$5-$9.99','$10-$14.99','$15-$19.99','$20-$29.99','$30-$39.99','$40-$59.99','$60+']\n",
    "        alldata['price_bucket'] = pd.cut(alldata['target_price'], bins=pricebreaks, labels=plabels, right=True)\n",
    "        alldata['price_bucket_id'] = pd.cut(alldata['target_price'], bins=pricebreaks, labels=False, right=True)\n",
    "        print(\"made price buckets\")\n",
    "\n",
    "    firstrows = len(alldata)\n",
    "    if 'target_price' in alldata.columns:\n",
    "        alldata.dropna(subset=['target_price'], inplace=True)\n",
    "        alldata = alldata[(alldata['target_price']>0) & (alldata['target_price']<=200)]\n",
    "        droprows = firstrows - len(alldata)\n",
    "        if droprows>0:\n",
    "            print(\"dropped \" + str(droprows) + \" rows no target or invalid price\")\n",
    "    else:\n",
    "        print(\"no target col found\")\n",
    "\n",
    "    alldata.reset_index(drop=True, inplace=True)\n",
    "    print(\"Preproc final shape is \" + str(alldata.shape))\n",
    "else:\n",
    "    print(\"df is empty\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5f457a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3) Basic Exploration\n",
      "============================================\n",
      "Some price stats:\n",
      "Num of games: 65109\n",
      "Median price: 6.99\n",
      "Mean price: 10.20861278625075\n",
      "25th: 3.99  75th: 12.99\n",
      "some correlation with target price:\n",
      "target_price     1.000000\n",
      "hltb_single      0.115804\n",
      "meta_score       0.098041\n",
      "reviews_total    0.052872\n",
      "achievements    -0.019015\n",
      "Name: target_price, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"3) Basic Exploration\")\n",
    "print(\"============================================\")\n",
    "\n",
    "if not alldata.empty and 'target_price' in alldata.columns and alldata['target_price'].nunique()>0:\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.histplot(alldata['target_price'], bins=50, kde=True)\n",
    "    plt.title(\"Distribution of Game Prices\")\n",
    "    plt.xlabel(\"Price\")\n",
    "    plt.ylabel(\"Count of Games\")\n",
    "    plt.xlim(0, alldata['target_price'].quantile(0.99))\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_price_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    sns.histplot(alldata['log_price'], bins=50, kde=True)\n",
    "    plt.title(\"Distribution of Log Price\")\n",
    "    plt.xlabel(\"Log Price\")\n",
    "    plt.ylabel(\"Count of Games\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_log_price_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "    plt.figure(figsize=(12,6))\n",
    "    pcounts = alldata['price_bucket'].value_counts().sort_index()\n",
    "    ax = sns.barplot(x=pcounts.index, y=pcounts.values)\n",
    "    plt.title(\"Price Bucket Distribution\")\n",
    "    plt.xlabel(\"Range\")\n",
    "    plt.ylabel(\"Count\")\n",
    "    plt.xticks(rotation=45)\n",
    "    for i, val in enumerate(pcounts.values):\n",
    "        ax.text(i, val+100, str(val), ha='center')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('eda_price_bucket_distribution.png')\n",
    "    plt.close()\n",
    "\n",
    "    print(\"Some price stats:\")\n",
    "    print(\"Num of games: \" + str(len(alldata)))\n",
    "    print(\"Median price: \" + str(alldata['target_price'].median()))\n",
    "    print(\"Mean price: \" + str(alldata['target_price'].mean()))\n",
    "    print(\"25th: \" + str(alldata['target_price'].quantile(0.25)) + \"  75th: \" + str(alldata['target_price'].quantile(0.75)))\n",
    "\n",
    "    fewcols = ['target_price']\n",
    "    maybe_cols = ['reviews_score_fancy','meta_score','revenue_estimated','reviews_total','achievements','hltb_single']\n",
    "    for cc in maybe_cols:\n",
    "        if cc in alldata.columns and pd.api.types.is_numeric_dtype(alldata[cc]):\n",
    "            fewcols.append(cc)\n",
    "    if len(fewcols)>1:\n",
    "        cmat = alldata[fewcols].corr()['target_price'].sort_values(ascending=False)\n",
    "        print(\"some correlation with target price:\")\n",
    "        print(cmat)\n",
    "else:\n",
    "    print(\"not enough data to do EDA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ede58fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4) Feature Engineering\n",
      "==============================================\n"
     ]
    }
   ],
   "source": [
    "print(\"4) Feature Engineering\")\n",
    "print(\"==============================================\")\n",
    "\n",
    "def make_features(dftrain, dftest=None):\n",
    "    print(\"feature engineering for train size: \", len(dftrain), \" test size: \", (len(dftest) if dftest is not None else 0))\n",
    "    traincopy = dftrain.copy()\n",
    "    testcopy = dftest.copy() if dftest is not None else None\n",
    "\n",
    "    def test(f, *args, **kwargs):\n",
    "        if testcopy is not None:\n",
    "            return f(testcopy, *args, **kwargs)\n",
    "\n",
    "    enged = []\n",
    "\n",
    "    #dates\n",
    "    #processing dates\n",
    "    datecols = []\n",
    "    for c in ['release_date','published_store','published_meta','published_igdb']:\n",
    "        if c in traincopy.columns:\n",
    "            datecols.append(c)\n",
    "\n",
    "    if datecols:\n",
    "        for col in datecols:\n",
    "            traincopy[col] = pd.to_datetime(traincopy[col], errors='coerce', unit='s', origin='unix')\n",
    "            test(lambda df,x: df.__setitem__(x, pd.to_datetime(df[x], errors='coerce', unit='s', origin='unix')), col)\n",
    "\n",
    "        traincopy['earliest_release_date'] = traincopy[datecols].min(axis=1)\n",
    "        test(lambda df: df.__setitem__('earliest_release_date', df[datecols].min(axis=1)))\n",
    "\n",
    "        if traincopy['earliest_release_date'].notna().any():\n",
    "            now = datetime(2025,4,21)\n",
    "            traincopy['game_age_days'] = (now - traincopy['earliest_release_date']).dt.days\n",
    "            test(lambda df: df.__setitem__('game_age_days',(now - df['earliest_release_date']).dt.days))\n",
    "\n",
    "            negmask = traincopy['game_age_days']<0\n",
    "            if negmask.any():\n",
    "                print(\"fixing \" + str(negmask.sum()) + \" future release age to 0\")\n",
    "                traincopy.loc[negmask,'game_age_days'] = 0\n",
    "                if testcopy is not None and (testcopy['game_age_days']<0).any():\n",
    "                    testcopy.loc[testcopy['game_age_days']<0,'game_age_days'] = 0\n",
    "\n",
    "            traincopy['age_years'] = traincopy['game_age_days']/365.25\n",
    "            test(lambda df: df.__setitem__('age_years', df['game_age_days']/365.25))\n",
    "\n",
    "            traincopy['age_years_log'] = np.log1p(np.maximum(0, traincopy['age_years']))\n",
    "            test(lambda df: df.__setitem__('age_years_log', np.log1p(np.maximum(0, df['age_years']))))\n",
    "\n",
    "            traincopy['release_year'] = traincopy['earliest_release_date'].dt.year\n",
    "            traincopy['release_month'] = traincopy['earliest_release_date'].dt.month\n",
    "            traincopy['release_quarter'] = traincopy['earliest_release_date'].dt.quarter\n",
    "            traincopy['release_is_q4'] = (traincopy['release_quarter']==4).astype(int)\n",
    "            traincopy['is_new_release'] = (traincopy['game_age_days']<=90).astype(int)\n",
    "\n",
    "            test(lambda df: df.__setitem__('release_year', df['earliest_release_date'].dt.year))\n",
    "            test(lambda df: df.__setitem__('release_month', df['earliest_release_date'].dt.month))\n",
    "            test(lambda df: df.__setitem__('release_quarter', df['earliest_release_date'].dt.quarter))\n",
    "            test(lambda df: df.__setitem__('release_is_q4', (df['release_quarter']==4).astype(int)))\n",
    "            test(lambda df: df.__setitem__('is_new_release', (df['game_age_days']<=90).astype(int)))\n",
    "\n",
    "            enged.extend(['game_age_days','age_years','age_years_log','release_year','release_month','release_quarter','release_is_q4','is_new_release'])\n",
    "        else:\n",
    "            print(\"no valid dates found\")\n",
    "    else:\n",
    "        print(\"no date col found\")\n",
    "\n",
    "    #review scores\n",
    "    print(\"doing reviewscore normalization\")\n",
    "    rmap = {\n",
    "        'reviews_score_fancy':100,\n",
    "        'meta_score':100,\n",
    "        'meta_uscore':10,\n",
    "        'igdb_score':100,\n",
    "        'igdb_uscore':100,\n",
    "        'gfq_rating':5\n",
    "    }\n",
    "    normed = []\n",
    "    for col,maxv in rmap.items():\n",
    "        if col in traincopy.columns:\n",
    "            try:\n",
    "                if col=='reviews_score_fancy':\n",
    "                    s1 = traincopy[col].astype(str).str.replace('%','',regex=False).str.strip()\n",
    "                    traincopy[col] = pd.to_numeric(s1, errors='coerce')\n",
    "                    if testcopy is not None:\n",
    "                        testcopy[col] = pd.to_numeric(testcopy[col].astype(str).str.replace('%','',regex=False).str.strip(), errors='coerce')\n",
    "\n",
    "                ncol = col+\"_norm\"\n",
    "                traincopy[ncol] = (traincopy[col]/maxv).clip(0,1)\n",
    "                if testcopy is not None:\n",
    "                    testcopy[ncol] = (testcopy[col]/maxv).clip(0,1)\n",
    "\n",
    "                normed.append(ncol)\n",
    "                enged.append(ncol)\n",
    "\n",
    "                if col=='reviews_score_fancy':\n",
    "                    traincopy['is_highly_rated'] = (traincopy[ncol]>=0.8).astype(int)\n",
    "                    if testcopy is not None:\n",
    "                        testcopy['is_highly_rated'] = (testcopy[ncol]>=0.8).astype(int)\n",
    "                    enged.append('is_highly_rated')\n",
    "            except Exception as e:\n",
    "                print(\"issue with \" + col + \" \" + str(e))\n",
    "\n",
    "    if normed:\n",
    "        weightsdict = {\n",
    "            'reviews_score_fancy_norm':0.45,\n",
    "            'meta_score_norm':0.25,\n",
    "            'igdb_score_norm':0.2,\n",
    "            'meta_uscore_norm':0.05,\n",
    "            'igdb_uscore_norm':0.05\n",
    "        }\n",
    "        avw = {k:v for k,v in weightsdict.items() if k in traincopy.columns}\n",
    "        if avw:\n",
    "            ssum = sum(avw.values())\n",
    "            nweights = {k:(v/ssum) for k,v in avw.items()}\n",
    "            traincopy['combined_score'] = sum(traincopy[c].fillna(0)*w for c,w in nweights.items()).clip(0,1)\n",
    "            if testcopy is not None:\n",
    "                testcopy['combined_score'] = sum(testcopy[c].fillna(0)*w for c,w in nweights.items()).clip(0,1)\n",
    "            enged.append('combined_score')\n",
    "\n",
    "            if len(avw)>1:\n",
    "                scols = list(avw.keys())\n",
    "                traincopy['score_variance'] = traincopy[scols].var(axis=1)\n",
    "                traincopy['is_controversial'] = (traincopy['score_variance']>traincopy['score_variance'].quantile(0.75)).astype(int)\n",
    "                if testcopy is not None:\n",
    "                    testcopy['score_variance'] = testcopy[scols].var(axis=1)\n",
    "                    t75 = traincopy['score_variance'].quantile(0.75)\n",
    "                    testcopy['is_controversial'] = (testcopy['score_variance']>t75).astype(int)\n",
    "                enged.extend(['score_variance','is_controversial'])\n",
    "            print(\"made combined_score from the available review norms\")\n",
    "        else:\n",
    "            traincopy['combined_score'] = 0.5\n",
    "            if testcopy is not None:\n",
    "                testcopy['combined_score'] = 0.5\n",
    "            print(\"no weighted scores, setting combined_score=0.5\")\n",
    "            enged.append('combined_score')\n",
    "    else:\n",
    "        traincopy['combined_score'] = 0.5\n",
    "        if testcopy is not None:\n",
    "            testcopy['combined_score'] = 0.5\n",
    "        print(\"no normed scores found, setting combined_score=0.5\")\n",
    "        enged.append('combined_score')\n",
    "\n",
    "    print(\"Processing tags genres categories\")\n",
    "    tagfields = []\n",
    "    for f in ['tags','genres','categories']:\n",
    "        if f in traincopy.columns:\n",
    "            tagfields.append(f)\n",
    "\n",
    "    binflags = []\n",
    "    for fld in tagfields:\n",
    "        try:\n",
    "            traincopy[fld] = traincopy[fld].astype(str).fillna('')\n",
    "            if testcopy is not None:\n",
    "                testcopy[fld] = testcopy[fld].astype(str).fillna('')\n",
    "\n",
    "            ccount = fld+\"_count\"\n",
    "            traincopy[ccount] = traincopy[fld].apply(lambda x: len(str(x).split(',')))\n",
    "            if testcopy is not None:\n",
    "                testcopy[ccount] = testcopy[fld].apply(lambda x: len(str(x).split(',')))\n",
    "            enged.append(ccount)\n",
    "\n",
    "            if fld=='tags':\n",
    "                kp = {\n",
    "                    'has_dlc_keywords':r'DLC|Expansion|Content Pack',\n",
    "                    'has_free_keywords':r'Free to Play|Free|F2P',\n",
    "                    'has_premium_keywords':r'AAA|Premium|Quality',\n",
    "                    'has_indie_keywords':r'Indie|Small|Casual',\n",
    "                    'has_franchise_keywords':r'Franchise|Series|Sequel'\n",
    "                }\n",
    "                for kkk, pat in kp.items():\n",
    "                    traincopy[kkk] = traincopy[fld].str.contains(pat, case=False, na=False).astype(int)\n",
    "                    if testcopy is not None:\n",
    "                        testcopy[kkk] = testcopy[fld].str.contains(pat, case=False, na=False).astype(int)\n",
    "                    enged.append(kkk)\n",
    "\n",
    "            if fld in ['tags','genres']:\n",
    "                common_tags = ['Indie','Action','RPG','Strategy','Casual','Adventure','Simulation','Singleplayer','Multiplayer','Early Access','Free to Play','Open World','Puzzle','Story Rich','Shooter','Platformer','Racing']\n",
    "                for tg in common_tags:\n",
    "                    flcol = fld+\"_has_\"+tg.lower().replace(' ','_')\n",
    "                    traincopy[flcol] = traincopy[fld].str.contains(tg, case=False, na=False).astype(int)\n",
    "                    if testcopy is not None:\n",
    "                        testcopy[flcol] = testcopy[fld].str.contains(tg, case=False, na=False).astype(int)\n",
    "                    binflags.append(flcol)\n",
    "                    enged.append(flcol)\n",
    "        except Exception as e:\n",
    "            print(\"issue with \" + fld + \" \" + str(e))\n",
    "\n",
    "    print(\"made \" + str(len(binflags)) + \" flags from tags or genres\")\n",
    "\n",
    "    print(\"making compound features\")\n",
    "    try:\n",
    "        combos = 0\n",
    "        combofeat = [\n",
    "            {\n",
    "                'name':'is_action_rpg',\n",
    "                'condition':lambda df: (df['genres_has_rpg'] & df['genres_has_action']).astype(int)\n",
    "            },\n",
    "            {\n",
    "                'name':'is_indie_casual',\n",
    "                'condition':lambda df: (df['genres_has_indie'] & df['genres_has_casual']).astype(int)\n",
    "            },\n",
    "            {\n",
    "                'name':'is_strategy_sim',\n",
    "                'condition':lambda df: (df['genres_has_strategy'] & df['genres_has_simulation']).astype(int)\n",
    "            }\n",
    "        ]\n",
    "        for cdef in combofeat:\n",
    "            nm = cdef['name']\n",
    "            cond = cdef['condition']\n",
    "            needed = []\n",
    "            if 'action_rpg' in nm:\n",
    "                needed = ['genres_has_rpg','genres_has_action']\n",
    "            elif 'indie_casual' in nm:\n",
    "                needed = ['genres_has_indie','genres_has_casual']\n",
    "            elif 'strategy_sim' in nm:\n",
    "                needed = ['genres_has_strategy','genres_has_simulation']\n",
    "\n",
    "            if all(kk in traincopy.columns for kk in needed):\n",
    "                traincopy[nm] = cond(traincopy)\n",
    "                if testcopy is not None:\n",
    "                    testcopy[nm] = cond(testcopy)\n",
    "                combos+=1\n",
    "                enged.append(nm)\n",
    "        print(\"made \" + str(combos) + \" compound feats\")\n",
    "    except Exception as e:\n",
    "        print(\"error making combos \" + str(e))\n",
    "\n",
    "    print(\"processing developer publisher\")\n",
    "    for fff in [f for f in ['developers','publishers'] if f in traincopy.columns]:\n",
    "        try:\n",
    "            traincopy[fff] = traincopy[fff].astype(str).fillna('Unknown')\n",
    "            if testcopy is not None:\n",
    "                testcopy[fff] = testcopy[fff].astype(str).fillna('Unknown')\n",
    "\n",
    "            valcounts = traincopy[fff].value_counts()\n",
    "            for ddf in [traincopy, testcopy] if testcopy is not None else [traincopy]:\n",
    "                ddf[fff+'_game_count'] = ddf[fff].map(valcounts).fillna(1).astype(int)\n",
    "                ddf[fff+'_is_prolific'] = (ddf[fff+'_game_count']>10).astype(int)\n",
    "                ddf[fff+'_is_major'] = (ddf[fff+'_game_count']>50).astype(int)\n",
    "                ddf[fff+'_log_games'] = np.log1p(ddf[fff+'_game_count'])\n",
    "\n",
    "            enged.extend([fff+'_game_count', fff+'_is_prolific', fff+'_is_major', fff+'_log_games'])\n",
    "\n",
    "            bigcos = [\n",
    "                'Ubisoft','Electronic Arts','EA','Activision','Blizzard','Square Enix','Capcom','Bethesda','Microsoft','Sony','Nintendo','Take-Two','2K Games','Rockstar','SEGA','Warner Bros','Bandai Namco','CD Projekt','Epic Games','THQ','Paradox','Deep Silver'\n",
    "            ]\n",
    "            traincopy[fff+'_is_aaa'] = traincopy[fff].apply(lambda x: 1 if any(b.lower() in x.lower() for b in bigcos) else 0)\n",
    "            if testcopy is not None:\n",
    "                testcopy[fff+'_is_aaa'] = testcopy[fff].apply(lambda x: 1 if any(b.lower() in x.lower() for b in bigcos) else 0)\n",
    "            enged.append(fff+'_is_aaa')\n",
    "        except Exception as e:\n",
    "            print(\"error dev/pub \" + str(e))\n",
    "\n",
    "    print(\"processing numeric features\")\n",
    "    numz = ['reviews_total','achievements','stsp_owners','hltb_single','hltb_complete','revenue_estimated']\n",
    "    for cc in numz:\n",
    "        if cc in traincopy.columns:\n",
    "            traincopy[cc] = pd.to_numeric(traincopy[cc], errors='coerce')\n",
    "            if testcopy is not None:\n",
    "                testcopy[cc] = pd.to_numeric(testcopy[cc], errors='coerce')\n",
    "\n",
    "            logc = cc+\"_log\"\n",
    "            traincopy[logc] = np.log1p(np.maximum(0, traincopy[cc]))\n",
    "            if testcopy is not None:\n",
    "                testcopy[logc] = np.log1p(np.maximum(0, testcopy[cc]))\n",
    "            enged.append(logc)\n",
    "\n",
    "            if cc=='hltb_single':\n",
    "                binsy = [-1,2,5,10,20,40,float('inf')]\n",
    "                labsy = ['Very Short (<2h)','Short (2-5h)','Medium (5-10h)','Long (10-20h)','Very Long (20-40h)','Massive (40h+)']\n",
    "                traincopy['playtime_category'] = pd.cut(traincopy[cc], bins=binsy, labels=labsy)\n",
    "                if testcopy is not None:\n",
    "                    testcopy['playtime_category'] = pd.cut(testcopy[cc], bins=binsy, labels=labsy)\n",
    "\n",
    "            if cc=='achievements':\n",
    "                abins = [-1,10,25,50,100,float('inf')]\n",
    "                alabs = ['Few','Some','Average','Many','Massive']\n",
    "                traincopy['achievement_tier'] = pd.cut(traincopy[cc], bins=abins, labels=alabs)\n",
    "                traincopy['has_achievements'] = (traincopy[cc]>0).astype(int)\n",
    "                if testcopy is not None:\n",
    "                    testcopy['achievement_tier'] = pd.cut(testcopy[cc], bins=abins, labels=alabs)\n",
    "                    testcopy['has_achievements'] = (testcopy[cc]>0).astype(int)\n",
    "                enged.append('has_achievements')\n",
    "\n",
    "            mkcols = ['revenue_estimated','stsp_owners']\n",
    "            for xco in mkcols:\n",
    "                if xco in traincopy.columns:\n",
    "                    traincopy[xco] = pd.to_numeric(traincopy[xco], errors='coerce')\n",
    "                    if testcopy is not None:\n",
    "                        testcopy[xco] = pd.to_numeric(testcopy[xco], errors='coerce')\n",
    "\n",
    "                    tiercol = xco+\"_tier\"\n",
    "                    if traincopy[xco].notna().sum()>4:\n",
    "                        qzz = [0,0.25,0.5,0.75,1]\n",
    "                        labsz = ['Low','Medium-Low','Medium','High']\n",
    "                        nonz = traincopy[xco][traincopy[xco]>0]\n",
    "                        if len(nonz)>0:\n",
    "                            qvals = np.unique(np.percentile(nonz,[0,25,50,75,100]))\n",
    "                            if len(qvals)<2:\n",
    "                                qvals = np.linspace(nonz.min(), nonz.max(),5)\n",
    "                            traincopy[tiercol] = pd.cut(traincopy[xco], bins=qvals, labels=labsz[:len(qvals)-1], include_lowest=True, duplicates='drop')\n",
    "                            if testcopy is not None:\n",
    "                                testcopy[tiercol] = pd.cut(testcopy[xco], bins=qvals, labels=labsz[:len(qvals)-1], include_lowest=True, duplicates='drop')\n",
    "                        else:\n",
    "                            print(\"not enough non zero in \" + xco)\n",
    "    \n",
    "    print(\"creating advanced feats now maybe\")\n",
    "    if all(x in traincopy.columns for x in ['combined_score','reviews_total_log']):\n",
    "        traincopy['score_x_log_reviews'] = traincopy['combined_score']*traincopy['reviews_total_log']\n",
    "        if testcopy is not None:\n",
    "            testcopy['score_x_log_reviews'] = testcopy['combined_score']*testcopy['reviews_total_log']\n",
    "        enged.append('score_x_log_reviews')\n",
    "\n",
    "    if all(x in traincopy.columns for x in ['combined_score','age_years_log']):\n",
    "        traincopy['score_x_age'] = traincopy['combined_score']*traincopy['age_years_log']\n",
    "        if testcopy is not None:\n",
    "            testcopy['score_x_age'] = testcopy['combined_score']*testcopy['age_years_log']\n",
    "        enged.append('score_x_age')\n",
    "\n",
    "    if all(x in traincopy.columns for x in ['game_age_days','reviews_total']):\n",
    "        tmedian = traincopy['game_age_days'][traincopy['game_age_days']>0].median()\n",
    "        if pd.isna(tmedian) or tmedian<=0:\n",
    "            tmedian = 365\n",
    "        def revperday(df, med):\n",
    "            return df.apply(lambda row: row['reviews_total']/row['game_age_days'] if pd.notna(row['game_age_days']) and row['game_age_days']>30 else row['reviews_total']/med if pd.notna(row['reviews_total']) else 0, axis=1).fillna(0)\n",
    "        traincopy['reviews_per_day'] = revperday(traincopy, tmedian)\n",
    "        if testcopy is not None:\n",
    "            testcopy['reviews_per_day'] = revperday(testcopy, tmedian)\n",
    "\n",
    "        traincopy['reviews_per_day_log'] = np.log1p(traincopy['reviews_per_day'])\n",
    "        if testcopy is not None:\n",
    "            testcopy['reviews_per_day_log'] = np.log1p(testcopy['reviews_per_day'])\n",
    "        enged.extend(['reviews_per_day','reviews_per_day_log'])\n",
    "\n",
    "        traincopy['recent_popularity'] = traincopy['reviews_per_day'] * np.minimum(1, 365/traincopy['game_age_days'].clip(lower=30))\n",
    "        if testcopy is not None:\n",
    "            testcopy['recent_popularity'] = testcopy['reviews_per_day'] * np.minimum(1, 365/testcopy['game_age_days'].clip(lower=30))\n",
    "        enged.append('recent_popularity')\n",
    "\n",
    "    qcomponents = ['combined_score','achievements','hltb_single']\n",
    "    qavail = [c for c in qcomponents if c in traincopy.columns]\n",
    "    if qavail:\n",
    "        traincopy['quality_indicator'] = 0\n",
    "        if testcopy is not None:\n",
    "            testcopy['quality_indicator'] = 0\n",
    "        compcount = 0\n",
    "        if 'combined_score' in qavail:\n",
    "            traincopy['quality_indicator'] += traincopy['combined_score']\n",
    "            if testcopy is not None:\n",
    "                testcopy['quality_indicator'] += testcopy['combined_score']\n",
    "            compcount+=1\n",
    "        if 'achievements' in qavail:\n",
    "            ascore = np.clip(traincopy['achievements']/100, 0,1)\n",
    "            traincopy['quality_indicator'] += ascore\n",
    "            if testcopy is not None:\n",
    "                testtest = np.clip(testcopy['achievements']/100,0,1)\n",
    "                testcopy['quality_indicator'] += testtest\n",
    "            compcount+=1\n",
    "        if 'hltb_single' in qavail:\n",
    "            tmaxp = max(40, traincopy['hltb_single'].quantile(0.95))\n",
    "            lscore = np.clip(traincopy['hltb_single']/tmaxp,0,1)\n",
    "            traincopy['quality_indicator'] += lscore\n",
    "            if testcopy is not None:\n",
    "                testlscore = np.clip(testcopy['hltb_single']/tmaxp,0,1)\n",
    "                testcopy['quality_indicator'] += testlscore\n",
    "            compcount+=1\n",
    "        if compcount>0:\n",
    "            traincopy['quality_indicator'] /= compcount\n",
    "            if testcopy is not None:\n",
    "                testcopy['quality_indicator'] /= compcount\n",
    "            enged.append('quality_indicator')\n",
    "            print(\"made quality_indicator from \" + str(compcount) + \" feats\")\n",
    "\n",
    "    print(\"Free or cheap games feature\")\n",
    "    pcol = None\n",
    "    if 'target_price' in traincopy.columns:\n",
    "        pcol = 'target_price'\n",
    "    elif 'launch_price' in traincopy.columns:\n",
    "        pcol = 'launch_price'\n",
    "    elif 'steamdb_full_price' in traincopy.columns:\n",
    "        pcol = 'steamdb_full_price'\n",
    "\n",
    "    if pcol:\n",
    "        cheapmask = traincopy[pcol]<1\n",
    "        if cheapmask.any():\n",
    "            print(\"There are \" + str(cheapmask.sum()) + \" free or cheap titles in train data\")\n",
    "            traincopy['is_free_or_cheap'] = cheapmask.astype(int)\n",
    "            if testcopy is not None:\n",
    "                testcopy['is_free_or_cheap'] = (testcopy[pcol]<1).astype(int)\n",
    "\n",
    "            if 'tags' in traincopy.columns:\n",
    "                microk = ['Free to Play','Microtransactions','In-App Purchases','F2P']\n",
    "                traincopy['has_microtransactions'] = traincopy['tags'].apply(lambda x: 1 if any(k.lower() in str(x).lower() for k in microk) else 0)\n",
    "                if testcopy is not None:\n",
    "                    testcopy['has_microtransactions'] = testcopy['tags'].apply(lambda x: 1 if any(k.lower() in str(x).lower() for k in microk) else 0)\n",
    "                enged.append('has_microtransactions')\n",
    "    else:\n",
    "        print(\"No price col to check free or cheap\")\n",
    "        traincopy['is_free_or_cheap'] = 0\n",
    "        if testcopy is not None:\n",
    "            testcopy['is_free_or_cheap'] = 0\n",
    "    enged.append('is_free_or_cheap')\n",
    "\n",
    "    tnum = traincopy.select_dtypes(include=np.number).columns\n",
    "    infmask = np.isinf(traincopy[tnum]).any(axis=1)\n",
    "    if infmask.any():\n",
    "        print(\"replacing \" + str(infmask.sum()) + \" infinite with nan in train\")\n",
    "        traincopy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "        if testcopy is not None:\n",
    "            testcopy.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "\n",
    "    for cl in tnum:\n",
    "        if traincopy[cl].isna().any():\n",
    "            tmed = traincopy[cl].median()\n",
    "            if pd.notna(tmed):\n",
    "                traincopy[cl] = traincopy[cl].fillna(tmed)\n",
    "                if testcopy is not None and cl in testcopy.columns:\n",
    "                    testcopy[cl] = testcopy[cl].fillna(tmed)\n",
    "            else:\n",
    "                traincopy[cl] = traincopy[cl].fillna(0)\n",
    "                if testcopy is not None and cl in testcopy.columns:\n",
    "                    testcopy[cl] = testcopy[cl].fillna(0)\n",
    "\n",
    "    print(\"Checking low variance features\")\n",
    "    lowvar = []\n",
    "    for c in tnum:\n",
    "        if c in ['target_price','log_price','price_bucket_id']:\n",
    "            continue\n",
    "        if traincopy[c].nunique()<=1:\n",
    "            lowvar.append(c)\n",
    "    if lowvar:\n",
    "        print(\"found \" + str(len(lowvar)) + \" features no variance: \" + str(lowvar[:10]))\n",
    "\n",
    "    print(\"Done with Feature Engineering. Made \" + str(len(enged)) + \" feats\")\n",
    "    goodfeats = [f for f in enged if f not in lowvar]\n",
    "    return traincopy, testcopy, goodfeats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28d3bf37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5) Models from Scratch\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"5) Models from Scratch\")\n",
    "print(\"==================================================\")\n",
    "\n",
    "#ridge reg\n",
    "def standardize(X):\n",
    "    X = np.asarray(X)\n",
    "    if X.ndim==1:\n",
    "        X = X.reshape(-1,1)\n",
    "    if X.shape[1]<=1:\n",
    "        return X, np.zeros(0), np.ones(0)\n",
    "    means = np.mean(X[:,1:], axis=0)\n",
    "    stds = np.std(X[:,1:], axis=0)+1e-7\n",
    "    Xn = (X[:,1:] - means)/stds\n",
    "    return np.hstack((X[:,:1], Xn)), means, stds\n",
    "\n",
    "def RidgePredict(X, W, means=None, stds=None):\n",
    "    X = np.asarray(X)\n",
    "    n,e = X.shape\n",
    "    XX = np.append(np.ones((n,1)), X, axis=1)\n",
    "    if means is not None and stds is not None:\n",
    "        Xnf = (XX[:,1:] - means)/stds\n",
    "        Xready = np.hstack((XX[:,:1], Xnf))\n",
    "    else:\n",
    "        Xready,_,_ = standardize(XX)\n",
    "    return Xready.dot(W)\n",
    "\n",
    "def RidgeUpdate(X, Y, W, lr, lam):\n",
    "    X, Y, W = np.asarray(X), np.asarray(Y), np.asarray(W)\n",
    "    n,e = X.shape\n",
    "    if n==0:\n",
    "        return W, np.inf\n",
    "    XX = np.append(np.ones((n,1)), X, axis=1)\n",
    "    Xn, mm, ss = standardize(XX)\n",
    "    yp = Xn.dot(W)\n",
    "    err = Y - yp\n",
    "    dwmse = -2*Xn.T.dot(err)/n\n",
    "    dwl2 = 2*lam*W\n",
    "    dwl2[0] = 0\n",
    "    dw = dwmse + dwl2\n",
    "    Wnew = W - lr*dw\n",
    "    newy = Xn.dot(Wnew)\n",
    "    cost = np.sqrt(np.mean((Y - newy)**2))\n",
    "    return Wnew, cost\n",
    "\n",
    "def RidgeFit(X, Y, lr=0.01, numiter=1000, lam=0.1, verbose=False, pstep=100, early_stop=True, pat=20, tol=1e-4):\n",
    "    X, Y = np.asarray(X), np.asarray(Y)\n",
    "    n,e = X.shape\n",
    "    Xb = np.append(np.ones((n,1)), X, axis=1)\n",
    "    Xn, mus, sigs = standardize(Xb)\n",
    "    W = np.zeros(e+1)\n",
    "    costhist = []\n",
    "    bestc = np.inf\n",
    "    bestw = W.copy()\n",
    "    ps=0\n",
    "    lrr=lr\n",
    "    for i in range(numiter):\n",
    "        W, cost = RidgeUpdate(X, Y, W, lrr, lam)\n",
    "        costhist.append(cost)\n",
    "        if early_stop:\n",
    "            if cost<bestc - tol:\n",
    "                bestc = cost\n",
    "                bestw = W.copy()\n",
    "                ps=0\n",
    "            else:\n",
    "                ps+=1\n",
    "            if ps>=pat:\n",
    "                lrr*=0.5\n",
    "                ps=0\n",
    "                if verbose:\n",
    "                    print(\"reducing learning rate now -> \", lrr)\n",
    "                if lrr<lr*0.01:\n",
    "                    if verbose:\n",
    "                        print(\" stopping early at i= \", i+1)\n",
    "                    break\n",
    "        if verbose and (i+1)%pstep==0:\n",
    "            print(\"iter= \", i+1, \" cost= \", cost, \" lr= \", lrr)\n",
    "        if np.isnan(cost) or np.isinf(cost):\n",
    "            print(\"cost is messed up at i= \", i+1)\n",
    "            break\n",
    "    if early_stop and bestc<cost:\n",
    "        if verbose:\n",
    "            print(\"using best weights cost= \", bestc)\n",
    "        return bestw, costhist, mus, sigs\n",
    "    return W, costhist, mus, sigs\n",
    "\n",
    "#Gradient boosting\n",
    "def boostIteration(X, y, H, eta, maxleaf=8, minleaf=1, rseed=42, sw=None):\n",
    "    X, y, H = np.asarray(X), np.asarray(y), np.asarray(H)\n",
    "    r = y-H\n",
    "    tree = DecisionTreeRegressor(\n",
    "        max_leaf_nodes=maxleaf,\n",
    "        min_samples_leaf=minleaf,\n",
    "        random_state=rseed\n",
    "    )\n",
    "    if sw is not None:\n",
    "        tree.fit(X, r, sample_weight=sw)\n",
    "    else:\n",
    "        tree.fit(X, r)\n",
    "    hh = tree.predict(X)\n",
    "    newH = H + eta*hh\n",
    "    return newH, tree\n",
    "\n",
    "def GBfit(X, y, n=100, lr=0.1, maxleaf=8, minleaf=1, verbose=False, pstep=10, Xv=None, yv=None, esr=10, usew=False):\n",
    "    X,y = np.asarray(X), np.asarray(y)\n",
    "    initp = np.mean(y)\n",
    "    H = np.full(y.shape[0], initp)\n",
    "    trees = []\n",
    "    bestval = np.inf\n",
    "    bestit = 0\n",
    "    nocount = 0\n",
    "    if usew:\n",
    "        sw = np.ones(y.shape[0])/y.shape[0]\n",
    "    else:\n",
    "        sw = None\n",
    "    for i in range(n):\n",
    "        H, t = boostIteration(X, y, H, lr, maxleaf, minleaf, rseed=seed+i, sw=sw)\n",
    "        trees.append(t)\n",
    "        trrmse = np.sqrt(np.mean((y-H)**2))\n",
    "        if usew:\n",
    "            eabs = np.abs(y-H)\n",
    "            sw = eabs/np.sum(eabs)\n",
    "        if Xv is not None and yv is not None:\n",
    "            vpred = GBpredict(Xv, initp, trees, lr)\n",
    "            vrmse = np.sqrt(np.mean((yv - vpred)**2))\n",
    "            if vrmse<bestval-1e-4:\n",
    "                bestval=vrmse\n",
    "                bestit=i\n",
    "                nocount=0\n",
    "            else:\n",
    "                nocount+=1\n",
    "            if nocount>=esr:\n",
    "                if verbose:\n",
    "                    print(\"earlystop at iteration= \", i+1, \" best iteration= \", bestit+1)\n",
    "                trees=trees[:bestit+1]\n",
    "                break\n",
    "            if verbose and (i+1)%pstep==0:\n",
    "                print(\" iter= \", i+1, \"/\", n, \" train rmse= \", trrmse, \" val rmse= \", vrmse)\n",
    "        else:\n",
    "            if verbose and (i+1)%pstep==0:\n",
    "                print(\" iter= \", i+1, \"/\", n, \" train rmse= \", trrmse)\n",
    "    return initp, trees, lr\n",
    "\n",
    "def GBpredict(X, initp, trees, lr):\n",
    "    X = np.asarray(X)\n",
    "    yp = np.full(X.shape[0], initp)\n",
    "    for t in trees:\n",
    "        yp += lr*t.predict(X)\n",
    "    return yp\n",
    "\n",
    "#feat selection\n",
    "def select_features(X,y, method='importance', n_features=None, est=None):\n",
    "    if method=='none':\n",
    "        return X, np.arange(X.shape[1])\n",
    "    if n_features is None:\n",
    "        n_features = min(int(X.shape[1]*0.7), 100)\n",
    "    if est is None:\n",
    "        est = DecisionTreeRegressor(max_depth=5, random_state=seed)\n",
    "    if method=='importance':\n",
    "        selector = SelectFromModel(est, threshold='median', max_features=n_features)\n",
    "        Xs = selector.fit_transform(X,y)\n",
    "        idx = np.where(selector.get_support())[0]\n",
    "    elif method=='rfe':\n",
    "        selector = RFE(est, n_features_to_select=n_features, step=0.2)\n",
    "        Xs = selector.fit_transform(X,y)\n",
    "        idx = np.where(selector.get_support())[0]\n",
    "    else:\n",
    "        raise ValueError(\"unknown method \" + method)\n",
    "    print(\"selected \" + str(Xs.shape[1]) + \" feats using \" + method)\n",
    "    return Xs, idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bec7f0d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6) Training Models\n",
      "===========================================\n",
      "Making base feats and targets\n",
      "split train test 80 20 with stratify on bucket\n",
      "train size=  52087  test size=  13022\n",
      "feature engineering separately for train test\n",
      "feature engineering for train size:  52087  test size:  13022\n",
      "no valid dates found\n",
      "doing reviewscore normalization\n",
      "made combined_score from the available review norms\n",
      "Processing tags genres categories\n",
      "made 34 flags from tags or genres\n",
      "making compound features\n",
      "made 3 compound feats\n",
      "processing developer publisher stuff\n",
      "processing numeric features\n",
      "creating advanced feats now maybe\n",
      "made quality_indicator from 3 feats\n",
      "Free or cheap games feature\n",
      "There are 4125 free or cheap titles in train data\n",
      "Checking low variance features\n",
      "found 14 features no variance: ['reviews_d7', 'reviews_d30', 'reviews_d90', 'name_slug', 'revenue_estimated', 'has_dlc_keywords', 'has_premium_keywords', 'genres_has_singleplayer', 'genres_has_open_world', 'genres_has_puzzle']\n",
      "Done with Feature Engineering. Made 76 feats\n",
      "got 67 eng feats in total\n",
      "trying 5-fold strat cv now\n",
      "feature selection approach maybe\n",
      "train ridge models\n",
      "training ridge for:  Price\n",
      "model=  MLR_Ridge_Price_importance  params=  {'lr': 0.005, 'iterations': 1000, 'lambda': 0.5, 'early_stopping': True}\n",
      "fold=  1  /  5\n",
      "doing  importance  selection\n",
      "selected 33 feats using importance\n",
      "iter=  200  cost=  9.76864052521681  lr=  0.005\n",
      "iter=  400  cost=  9.663021255274524  lr=  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "iter=  600  cost=  9.660030401647006  lr=  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      "reducing learning rate now ->  3.90625e-05\n",
      " stopping early at i=  699\n",
      "fold metrics= rmse=  9.569224383187795  mae=  5.708367017963352  mape=  104.91762137288183  r2=  0.2172868707632417\n",
      "fold=  2  /  5\n",
      "doing  importance  selection\n",
      "selected 33 feats using importance\n",
      "iter=  200  cost=  9.821172944421184  lr=  0.005\n",
      "iter=  400  cost=  9.709556090849205  lr=  0.005\n",
      "iter=  600  cost=  9.705023217023687  lr=  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      "reducing learning rate now ->  3.90625e-05\n",
      " stopping early at i=  775\n",
      "fold metrics= rmse=  9.46697437900175  mae=  5.6704391359904545  mape=  105.80965460797134  r2=  0.2301596541284896\n",
      "fold=  3  /  5\n",
      "doing  importance  selection\n",
      "selected 33 feats using importance\n",
      "iter=  200  cost=  9.732956114961354  lr=  0.005\n",
      "iter=  400  cost=  9.620565985121301  lr=  0.005\n",
      "iter=  600  cost=  9.616150340510467  lr=  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      "reducing learning rate now ->  3.90625e-05\n",
      " stopping early at i=  756\n",
      "fold metrics= rmse=  9.742240970814235  mae=  5.680486512146964  mape=  104.78452226073263  r2=  0.2178431590200618\n",
      "fold=  4  /  5\n",
      "doing  importance  selection\n",
      "selected 33 feats using importance\n",
      "iter=  200  cost=  9.744746037009053  lr=  0.005\n",
      "iter=  400  cost=  9.639484145942138  lr=  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "iter=  600  cost=  9.63651822360057  lr=  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      "reducing learning rate now ->  3.90625e-05\n",
      " stopping early at i=  706\n",
      "fold metrics= rmse=  9.777874648079912  mae=  5.698742195496099  mape=  104.21966026587084  r2=  0.21475687807925303\n",
      "fold=  5  /  5\n",
      "doing  importance  selection\n",
      "selected 33 feats using importance\n",
      "iter=  200  cost=  9.715135513195644  lr=  0.005\n",
      "iter=  400  cost=  9.602357017563973  lr=  0.005\n",
      "iter=  600  cost=  9.598024442872129  lr=  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      "reducing learning rate now ->  3.90625e-05\n",
      " stopping early at i=  757\n",
      "fold metrics= rmse=  9.668897060846785  mae=  5.640405305878767  mape=  102.74321824483262  r2=  0.21692910339823013\n",
      "cv average= rmse=  9.645042288386097  mae=  5.679688033495127  mape=  104.49493535045785  r2=  0.21939513307785524\n",
      "model=  MLR_Ridge_Price  params=  {'lr': 0.005, 'iterations': 1000, 'lambda': 0.5, 'early_stopping': True}\n",
      "fold=  1  /  5\n",
      "iter=  200  cost=  9.654053250825548  lr=  0.005\n",
      "iter=  400  cost=  9.539548876093015  lr=  0.005\n",
      "iter=  600  cost=  9.534813930813261  lr=  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      "reducing learning rate now ->  3.90625e-05\n",
      " stopping early at i=  790\n",
      "fold metrics= rmse=  9.44200294435189  mae=  5.583495459616776  mape=  101.93170901478972  r2=  0.23796063695159808\n",
      "fold=  2  /  5\n",
      "iter=  200  cost=  9.678740781398798  lr=  0.005\n",
      "iter=  400  cost=  9.563193757671932  lr=  0.005\n",
      "iter=  600  cost=  9.558205266811303  lr=  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      "reducing learning rate now ->  3.90625e-05\n",
      " stopping early at i=  775\n",
      "fold metrics= rmse=  9.33320314810917  mae=  5.539816441861583  mape=  102.33543659401398  r2=  0.25176210060980875\n",
      "fold=  3  /  5\n",
      "iter=  200  cost=  9.607534060162688  lr=  0.005\n",
      "iter=  400  cost=  9.492239772284861  lr=  0.005\n",
      "iter=  600  cost=  9.487421451666508  lr=  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      "reducing learning rate now ->  3.90625e-05\n",
      " stopping early at i=  779\n",
      "fold metrics= rmse=  9.629573289108913  mae=  5.580235279346704  mape=  102.05441926175169  r2=  0.2358296219790743\n",
      "fold=  4  /  5\n",
      "iter=  200  cost=  9.607104321862861  lr=  0.005\n",
      "iter=  400  cost=  9.491967382514233  lr=  0.005\n",
      "iter=  600  cost=  9.487190974546237  lr=  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      "reducing learning rate now ->  3.90625e-05\n",
      " stopping early at i=  791\n",
      "fold metrics= rmse=  9.633173546508639  mae=  5.574245669179939  mape=  101.73776636496115  r2=  0.23782626406759588\n",
      "fold=  5  /  5\n",
      "iter=  200  cost=  9.623734250807514  lr=  0.005\n",
      "iter=  400  cost=  9.509127699495217  lr=  0.005\n",
      "iter=  600  cost=  9.504435394949303  lr=  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      "reducing learning rate now ->  3.90625e-05\n",
      " stopping early at i=  768\n",
      "fold metrics= rmse=  9.567790751678865  mae=  5.562889318625952  mape=  100.8162481904681  r2=  0.23322040426295343\n",
      "cv average= rmse=  9.521148735951495  mae=  5.56813643372619  mape=  101.77511588519693  r2=  0.23931980557420607\n",
      "train final on full training set no selection\n",
      "iter=  200  cost=  9.635259781480174  lr=  0.005\n",
      "iter=  400  cost=  9.520331535107086  lr=  0.005\n",
      "iter=  600  cost=  9.515545034752012  lr=  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      "reducing learning rate now ->  3.90625e-05\n",
      " stopping early at i=  790\n",
      "bucket acc=  29.657502687759173  bucket acc ±1=  79.21978190754109\n",
      "saved final model=  models\\mlr_ridge_price_final.joblib\n",
      "training ridge for:  LogPrice\n",
      "model=  MLR_Ridge_LogPrice_importance  params=  {'lr': 0.01, 'iterations': 800, 'lambda': 0.1, 'early_stopping': True}\n",
      "fold=  1  /  5\n",
      "doing  importance  selection\n",
      "selected 33 feats using importance\n",
      "iter=  200  cost=  0.5910279854430798  lr=  0.01\n",
      "reducing learning rate now ->  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "iter=  400  cost=  0.589413638345803  lr=  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      " stopping early at i=  426\n",
      "fold metrics= rmse=  0.5906709813734393  mae=  0.46650394631048064  mape=  25.7780839211731  r2=  0.43771957127921335\n",
      "fold=  2  /  5\n",
      "doing  importance  selection\n",
      "selected 33 feats using importance\n",
      "iter=  200  cost=  0.5912720004468627  lr=  0.01\n",
      "reducing learning rate now ->  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "iter=  400  cost=  0.5896648033783503  lr=  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      " stopping early at i=  429\n",
      "fold metrics= rmse=  0.5893410516233499  mae=  0.465720823803079  mape=  25.809363613388093  r2=  0.44589044506934916\n",
      "fold=  3  /  5\n",
      "doing  importance  selection\n",
      "selected 33 feats using importance\n",
      "iter=  200  cost=  0.5916832602497228  lr=  0.01\n",
      "reducing learning rate now ->  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "iter=  400  cost=  0.5894713718025426  lr=  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      " stopping early at i=  455\n",
      "fold metrics= rmse=  0.5854248359932211  mae=  0.4596115482960114  mape=  25.394378273490393  r2=  0.4491919570239158\n",
      "fold=  4  /  5\n",
      "doing  importance  selection\n",
      "selected 33 feats using importance\n",
      "iter=  200  cost=  0.5915617568671722  lr=  0.01\n",
      "reducing learning rate now ->  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "iter=  400  cost=  0.5899394166296282  lr=  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      " stopping early at i=  425\n",
      "fold metrics= rmse=  0.5966445045893427  mae=  0.4701965203510606  mape=  26.06690794501611  r2=  0.43539348240219133\n",
      "fold=  5  /  5\n",
      "doing  importance  selection\n",
      "selected 33 feats using importance\n",
      "iter=  200  cost=  0.5924880222326318  lr=  0.01\n",
      "reducing learning rate now ->  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "iter=  400  cost=  0.5908849670075619  lr=  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      " stopping early at i=  431\n",
      "fold metrics= rmse=  0.5901236153939295  mae=  0.4655827726339534  mape=  25.540653854652668  r2=  0.4400157152245523\n",
      "cv average= rmse=  0.5904409977946565  mae=  0.465523122278917  mape=  25.717877521544075  r2=  0.4416422341998444\n",
      "model=  MLR_Ridge_LogPrice  params=  {'lr': 0.01, 'iterations': 800, 'lambda': 0.1, 'early_stopping': True}\n",
      "fold=  1  /  5\n",
      "iter=  200  cost=  0.5805044392766938  lr=  0.01\n",
      "reducing learning rate now ->  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "iter=  400  cost=  0.5782555334752038  lr=  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      " stopping early at i=  462\n",
      "fold metrics= rmse=  0.58136655371291  mae=  0.4569067557736857  mape=  25.256830913493676  r2=  0.45529447201974993\n",
      "fold=  2  /  5\n",
      "iter=  200  cost=  0.5812737446756806  lr=  0.01\n",
      "reducing learning rate now ->  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "iter=  400  cost=  0.5790169510763268  lr=  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      " stopping early at i=  454\n",
      "fold metrics= rmse=  0.5779715190917729  mae=  0.4550689591565227  mape=  25.323840444023865  r2=  0.4670639143296159\n",
      "fold=  3  /  5\n",
      "iter=  200  cost=  0.5814496611578717  lr=  0.01\n",
      "reducing learning rate now ->  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "iter=  400  cost=  0.5791985520735259  lr=  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      " stopping early at i=  460\n",
      "fold metrics= rmse=  0.5773823961174427  mae=  0.4526404896899321  mape=  25.026607741779728  r2=  0.46422176854561126\n",
      "fold=  4  /  5\n",
      "iter=  200  cost=  0.579711871785691  lr=  0.01\n",
      "reducing learning rate now ->  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "iter=  400  cost=  0.5774576939690159  lr=  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      " stopping early at i=  455\n",
      "fold metrics= rmse=  0.5842551633503068  mae=  0.45808665038443547  mape=  25.512545942619948  r2=  0.45859817463683694\n",
      "fold=  5  /  5\n",
      "iter=  200  cost=  0.5819843658328042  lr=  0.01\n",
      "reducing learning rate now ->  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "iter=  400  cost=  0.5797854105119287  lr=  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      " stopping early at i=  465\n",
      "fold metrics= rmse=  0.5760131531558444  mae=  0.4522353768037825  mape=  24.89389659769256  r2=  0.4664751510712808\n",
      "cv average= rmse=  0.5793977570856554  mae=  0.45498764636167166  mape=  25.202744327921955  r2=  0.46233069612061894\n",
      "train final on full training set no selection\n",
      "iter=  200  cost=  0.5810585435698657  lr=  0.01\n",
      "reducing learning rate now ->  0.005\n",
      "reducing learning rate now ->  0.0025\n",
      "reducing learning rate now ->  0.00125\n",
      "iter=  400  cost=  0.5788145252895018  lr=  0.00125\n",
      "reducing learning rate now ->  0.000625\n",
      "reducing learning rate now ->  0.0003125\n",
      "reducing learning rate now ->  0.00015625\n",
      "reducing learning rate now ->  7.8125e-05\n",
      " stopping early at i=  462\n",
      "test metrics on original scale=  {'RMSE (orig)': 9.457146664497563, 'MAE (orig)': 5.044728146238903, 'MAPE (%) (orig)': 63.14882139686713}\n",
      "bucket acc=  37.93580095223468  bucket acc ±1=  86.16955920749501\n",
      "saved final model=  models\\mlr_ridge_logprice_final.joblib\n",
      "Training gradient boosting from scratch\n",
      "Training GBDT for=  Price\n",
      "model=  GBDT_Price_importance  params=  {'n_estimators': 200, 'learning_rate': 0.05, 'max_leaves': 12, 'min_samples': 20, 'early_stopping': True, 'early_stopping_rounds': 10, 'use_sample_weights': True}\n",
      "fold=  1 / 5\n",
      "selection=  importance\n",
      "selected 33 feats using importance\n",
      " iter=  20 / 200  train rmse=  9.953864116233934  val rmse=  9.99516757806297\n",
      " iter=  40 / 200  train rmse=  9.676554923760309  val rmse=  9.74377871462433\n",
      " iter=  60 / 200  train rmse=  9.500833992974863  val rmse=  9.605978436280884\n",
      " iter=  80 / 200  train rmse=  9.380893842847204  val rmse=  9.526686765607497\n",
      " iter=  100 / 200  train rmse=  9.274614204601136  val rmse=  9.455685972021724\n",
      " iter=  120 / 200  train rmse=  9.202004125681274  val rmse=  9.42365514696613\n",
      " iter=  140 / 200  train rmse=  9.130130843668717  val rmse=  9.388116115539596\n",
      " iter=  160 / 200  train rmse=  9.062986016261355  val rmse=  9.353229879214632\n",
      " iter=  180 / 200  train rmse=  9.008250785416513  val rmse=  9.329183924669902\n",
      " iter=  200 / 200  train rmse=  8.949300567466778  val rmse=  9.303721738513122\n",
      "fold metrics= rmse=  9.303721738513122  mae=  6.440273366831611  mape=  128.8634698980987  r2=  0.26011781944025103\n",
      "fold=  2 / 5\n",
      "selection=  importance\n",
      "selected 33 feats using importance\n",
      " iter=  20 / 200  train rmse=  9.96659940704462  val rmse=  9.865311194573017\n",
      " iter=  40 / 200  train rmse=  9.682200859328992  val rmse=  9.676749690884263\n",
      " iter=  60 / 200  train rmse=  9.493735783594431  val rmse=  9.555604488214717\n",
      " iter=  80 / 200  train rmse=  9.355487241102022  val rmse=  9.494269868198396\n",
      " iter=  100 / 200  train rmse=  9.246564498315344  val rmse=  9.443168906104948\n",
      " iter=  120 / 200  train rmse=  9.15760816453502  val rmse=  9.416798770788349\n",
      " iter=  140 / 200  train rmse=  9.07879983208866  val rmse=  9.394230343636314\n",
      " iter=  160 / 200  train rmse=  9.01330869360287  val rmse=  9.372235463471695\n",
      " iter=  180 / 200  train rmse=  8.94545289731253  val rmse=  9.343897524773816\n",
      " iter=  200 / 200  train rmse=  8.87658956020164  val rmse=  9.312104751137888\n",
      "fold metrics= rmse=  9.312104751137888  mae=  6.418922941839585  mape=  131.94951074752615  r2=  0.2551411713463889\n",
      "fold=  3 / 5\n",
      "selection=  importance\n",
      "selected 33 feats using importance\n",
      " iter=  20 / 200  train rmse=  9.886322588324719  val rmse=  10.105733952375441\n",
      " iter=  40 / 200  train rmse=  9.619244243897894  val rmse=  9.943007569803463\n",
      " iter=  60 / 200  train rmse=  9.432274146310453  val rmse=  9.813235292766269\n",
      " iter=  80 / 200  train rmse=  9.302301368853229  val rmse=  9.733263170381315\n",
      " iter=  100 / 200  train rmse=  9.200968623058145  val rmse=  9.669516566288669\n",
      " iter=  120 / 200  train rmse=  9.118682912253927  val rmse=  9.638191913133275\n",
      " iter=  140 / 200  train rmse=  9.047587646834534  val rmse=  9.61623882894789\n",
      " iter=  160 / 200  train rmse=  8.981083215323233  val rmse=  9.584534413361148\n",
      " iter=  180 / 200  train rmse=  8.916662156307474  val rmse=  9.551928317249882\n",
      " iter=  200 / 200  train rmse=  8.859884834203637  val rmse=  9.530795736541535\n",
      "fold metrics= rmse=  9.530795736541535  mae=  6.478241356246632  mape=  129.9853442643475  r2=  0.251426520402554\n",
      "fold=  4 / 5\n",
      "selection=  importance\n",
      "selected 33 feats using importance\n",
      " iter=  20 / 200  train rmse=  9.893853056077296  val rmse=  10.088151657075818\n",
      " iter=  40 / 200  train rmse=  9.622838272778473  val rmse=  9.867915099726194\n",
      " iter=  60 / 200  train rmse=  9.464154541685204  val rmse=  9.76644050909357\n",
      " iter=  80 / 200  train rmse=  9.34316699216719  val rmse=  9.689029208733759\n",
      " iter=  100 / 200  train rmse=  9.253330041133017  val rmse=  9.642732604226754\n",
      " iter=  120 / 200  train rmse=  9.168444595190003  val rmse=  9.600963433557897\n",
      " iter=  140 / 200  train rmse=  9.094419498030502  val rmse=  9.555390053949322\n",
      " iter=  160 / 200  train rmse=  9.033957913178602  val rmse=  9.530219973588489\n",
      " iter=  180 / 200  train rmse=  8.96979720120509  val rmse=  9.50223308020318\n",
      " iter=  200 / 200  train rmse=  8.908456980246942  val rmse=  9.472767852441107\n",
      "fold metrics= rmse=  9.472767852441107  mae=  6.468061156351635  mape=  133.55160543222266  r2=  0.2629974357899726\n",
      "fold=  5 / 5\n",
      "selection=  importance\n",
      "selected 33 feats using importance\n",
      " iter=  20 / 200  train rmse=  9.898013471522189  val rmse=  10.048510613260646\n",
      " iter=  40 / 200  train rmse=  9.589861663981296  val rmse=  9.832175840144949\n",
      " iter=  60 / 200  train rmse=  9.384038540476933  val rmse=  9.693330060723511\n",
      " iter=  80 / 200  train rmse=  9.254044004090451  val rmse=  9.619303583429904\n",
      " iter=  100 / 200  train rmse=  9.145936425998624  val rmse=  9.56270265784349\n",
      " iter=  120 / 200  train rmse=  9.041652811629719  val rmse=  9.516471752471208\n",
      " iter=  140 / 200  train rmse=  8.970045852257627  val rmse=  9.489018444102893\n",
      " iter=  160 / 200  train rmse=  8.900895649464603  val rmse=  9.468046407162948\n",
      " iter=  180 / 200  train rmse=  8.836880962820873  val rmse=  9.438547381731805\n",
      " iter=  200 / 200  train rmse=  8.777582333473335  val rmse=  9.419246635165829\n",
      "fold metrics= rmse=  9.419246635165829  mae=  6.332851608219392  mape=  127.43741029680773  r2=  0.25684475441052634\n",
      "cv avg= rmse=  9.407727342759895  mae=  6.427670085897771  mape=  130.35746812780056  r2=  0.25730554027793856\n",
      "model=  GBDT_Price  params=  {'n_estimators': 200, 'learning_rate': 0.05, 'max_leaves': 12, 'min_samples': 20, 'early_stopping': True, 'early_stopping_rounds': 10, 'use_sample_weights': True}\n",
      "fold=  1 / 5\n",
      " iter=  20 / 200  train rmse=  9.832359301279508  val rmse=  9.914205903823134\n",
      " iter=  40 / 200  train rmse=  9.508119760533674  val rmse=  9.65117536628554\n",
      " iter=  60 / 200  train rmse=  9.290360071919592  val rmse=  9.47419532733664\n",
      " iter=  80 / 200  train rmse=  9.122538185923347  val rmse=  9.34982551691086\n",
      " iter=  100 / 200  train rmse=  8.991147111856815  val rmse=  9.244039141425203\n",
      " iter=  120 / 200  train rmse=  8.878536178973345  val rmse=  9.194459845092807\n",
      " iter=  140 / 200  train rmse=  8.787203092992737  val rmse=  9.158802415928976\n",
      " iter=  160 / 200  train rmse=  8.71598043685067  val rmse=  9.125715774001351\n",
      " iter=  180 / 200  train rmse=  8.64542057466942  val rmse=  9.087686889842159\n",
      " iter=  200 / 200  train rmse=  8.578279132558825  val rmse=  9.064164510358856\n",
      "fold metrics= rmse=  9.064164510358856  mae=  6.21536323685751  mape=  126.92306328632073  r2=  0.2977290551932599\n",
      "fold=  2 / 5\n",
      " iter=  20 / 200  train rmse=  9.8726966551471  val rmse=  9.812922263379459\n",
      " iter=  40 / 200  train rmse=  9.540890808090193  val rmse=  9.607792252505556\n",
      " iter=  60 / 200  train rmse=  9.29395339623821  val rmse=  9.44035351881764\n",
      " iter=  80 / 200  train rmse=  9.13561158291846  val rmse=  9.352217452500481\n",
      " iter=  100 / 200  train rmse=  8.997042574396707  val rmse=  9.283918014944085\n",
      " iter=  120 / 200  train rmse=  8.889779330092445  val rmse=  9.230160213255415\n",
      " iter=  140 / 200  train rmse=  8.805299592102436  val rmse=  9.197296325595815\n",
      " iter=  160 / 200  train rmse=  8.72182583588239  val rmse=  9.168313128392965\n",
      " iter=  180 / 200  train rmse=  8.635657442049103  val rmse=  9.132299883584661\n",
      " iter=  200 / 200  train rmse=  8.562608565243842  val rmse=  9.104260671903823\n",
      "fold metrics= rmse=  9.104260671903823  mae=  6.300798561518132  mape=  131.34508055695494  r2=  0.28802026563227046\n",
      "fold=  3 / 5\n",
      " iter=  20 / 200  train rmse=  9.78246684654763  val rmse=  10.000921665811514\n",
      " iter=  40 / 200  train rmse=  9.467959131861015  val rmse=  9.778207694305854\n",
      " iter=  60 / 200  train rmse=  9.235643714771157  val rmse=  9.602021059978924\n",
      " iter=  80 / 200  train rmse=  9.079192459988798  val rmse=  9.50177766061808\n",
      " iter=  100 / 200  train rmse=  8.979030339502701  val rmse=  9.447264541046845\n",
      " iter=  120 / 200  train rmse=  8.879252128784412  val rmse=  9.397415492294412\n",
      " iter=  140 / 200  train rmse=  8.774633881590228  val rmse=  9.329386449263623\n",
      " iter=  160 / 200  train rmse=  8.693119976694762  val rmse=  9.296666002896165\n",
      " iter=  180 / 200  train rmse=  8.617174682523732  val rmse=  9.25580542741121\n",
      " iter=  200 / 200  train rmse=  8.564827802148685  val rmse=  9.227843228470352\n",
      "fold metrics= rmse=  9.227843228470352  mae=  6.277563052904827  mape=  125.37168222679318  r2=  0.29825952168375736\n",
      "fold=  4 / 5\n",
      " iter=  20 / 200  train rmse=  9.815707376985225  val rmse=  9.954549984245324\n",
      " iter=  40 / 200  train rmse=  9.486875390534488  val rmse=  9.68863395198062\n",
      " iter=  60 / 200  train rmse=  9.24016204519045  val rmse=  9.497572855092411\n",
      " iter=  80 / 200  train rmse=  9.083665778272866  val rmse=  9.39302559084453\n",
      " iter=  100 / 200  train rmse=  8.951656950161412  val rmse=  9.313617505687487\n",
      " iter=  120 / 200  train rmse=  8.83126080416598  val rmse=  9.25041687856145\n",
      " iter=  140 / 200  train rmse=  8.745180859501152  val rmse=  9.213652912124628\n",
      " iter=  160 / 200  train rmse=  8.665560099529912  val rmse=  9.17966112463392\n",
      " iter=  180 / 200  train rmse=  8.584620275674657  val rmse=  9.138730624188154\n",
      " iter=  200 / 200  train rmse=  8.503968399357444  val rmse=  9.115052615451106\n",
      "fold metrics= rmse=  9.115052615451106  mae=  6.210283157792833  mape=  128.12537615141687  r2=  0.31760856026146156\n",
      "fold=  5 / 5\n",
      " iter=  20 / 200  train rmse=  9.795557549471232  val rmse=  9.979477774881195\n",
      " iter=  40 / 200  train rmse=  9.442700694150993  val rmse=  9.702005721545564\n",
      " iter=  60 / 200  train rmse=  9.210972005270694  val rmse=  9.556613919877758\n",
      " iter=  80 / 200  train rmse=  9.042891571573456  val rmse=  9.450658838572565\n",
      " iter=  100 / 200  train rmse=  8.924849142423886  val rmse=  9.386488191153878\n",
      " iter=  120 / 200  train rmse=  8.813659189133492  val rmse=  9.323696270827737\n",
      " iter=  140 / 200  train rmse=  8.7306978593859  val rmse=  9.286396276079547\n",
      " iter=  160 / 200  train rmse=  8.632210107477864  val rmse=  9.25639268087787\n",
      " iter=  180 / 200  train rmse=  8.552573327368249  val rmse=  9.220300816722403\n",
      " iter=  200 / 200  train rmse=  8.481575423934665  val rmse=  9.197701010216868\n",
      "fold metrics= rmse=  9.197701010216868  mae=  6.200543609294655  mape=  124.60407728335045  r2=  0.2913924326189896\n",
      "cv avg= rmse=  9.141804407280201  mae=  6.240910323673591  mape=  127.27385590096722  r2=  0.2986019670779478\n",
      "final training on full data now for  GBDT_Price\n",
      " iter=  20 / 200  train rmse=  9.76843703183386  val rmse=  10.17512636855626\n",
      " iter=  40 / 200  train rmse=  9.40664956537221  val rmse=  9.869162262394255\n",
      " iter=  60 / 200  train rmse=  9.180962598151472  val rmse=  9.714403358241382\n",
      " iter=  80 / 200  train rmse=  9.033862457662766  val rmse=  9.6176278051497\n",
      " iter=  100 / 200  train rmse=  8.902529943105485  val rmse=  9.544516799496797\n",
      " iter=  120 / 200  train rmse=  8.806532681643755  val rmse=  9.50159552272652\n",
      " iter=  140 / 200  train rmse=  8.707153496901883  val rmse=  9.443104601727422\n",
      " iter=  160 / 200  train rmse=  8.627489734122955  val rmse=  9.392203749035513\n",
      " iter=  180 / 200  train rmse=  8.54195546086068  val rmse=  9.363978325865716\n",
      " iter=  200 / 200  train rmse=  8.472537775071553  val rmse=  9.334115982812019\n",
      "bucket acc=  23.667639379511595  plusminus1=  65.68115496851482\n",
      "saved final model=  models\\gbdt_price_final.joblib\n",
      "Training GBDT for=  LogPrice\n",
      "model=  GBDT_LogPrice_importance  params=  {'n_estimators': 200, 'learning_rate': 0.03, 'max_leaves': 12, 'min_samples': 20, 'early_stopping': True, 'early_stopping_rounds': 10, 'use_sample_weights': False}\n",
      "fold=  1 / 5\n",
      "selection=  importance\n",
      "selected 33 feats using importance\n",
      " iter=  20 / 200  train rmse=  0.660990340732046  val rmse=  0.6629542289047365\n",
      " iter=  40 / 200  train rmse=  0.610875670798338  val rmse=  0.6146274352963317\n",
      " iter=  60 / 200  train rmse=  0.5891823310248794  val rmse=  0.5933391125911\n",
      " iter=  80 / 200  train rmse=  0.5789695869469079  val rmse=  0.5833585301708053\n",
      " iter=  100 / 200  train rmse=  0.573580740487509  val rmse=  0.578051933859416\n",
      " iter=  120 / 200  train rmse=  0.5702071266437051  val rmse=  0.5747787177215751\n",
      " iter=  140 / 200  train rmse=  0.5677125157346586  val rmse=  0.5723000551505252\n",
      " iter=  160 / 200  train rmse=  0.5658220577881404  val rmse=  0.5704644902824314\n",
      " iter=  180 / 200  train rmse=  0.5643886019427763  val rmse=  0.5692494366303383\n",
      " iter=  200 / 200  train rmse=  0.5630907652170067  val rmse=  0.5682462454538203\n",
      "fold metrics= rmse=  0.5682462454538203  mae=  0.4397200219226341  mape=  23.373433041274406  r2=  0.47960292529487636\n",
      "fold=  2 / 5\n",
      "selection=  importance\n",
      "selected 33 feats using importance\n",
      " iter=  20 / 200  train rmse=  0.6612557473587323  val rmse=  0.6615935691336163\n",
      " iter=  40 / 200  train rmse=  0.6113749824771536  val rmse=  0.6113919771259488\n",
      " iter=  60 / 200  train rmse=  0.5898187885870013  val rmse=  0.5897027973810238\n",
      " iter=  80 / 200  train rmse=  0.5796423577204536  val rmse=  0.5797188335685743\n",
      " iter=  100 / 200  train rmse=  0.5742546451884919  val rmse=  0.57461709536452\n",
      " iter=  120 / 200  train rmse=  0.5707495816619166  val rmse=  0.5714692219844115\n",
      " iter=  140 / 200  train rmse=  0.568277066993078  val rmse=  0.5694433541147949\n",
      " iter=  160 / 200  train rmse=  0.5663034784798028  val rmse=  0.5678706086673159\n",
      " iter=  180 / 200  train rmse=  0.5646443334437662  val rmse=  0.5665869979767715\n",
      " iter=  200 / 200  train rmse=  0.563233515303944  val rmse=  0.5655281265696859\n",
      "fold metrics= rmse=  0.5655281265696859  mae=  0.4370535485419373  mape=  23.416522534592453  r2=  0.4897645013050492\n",
      "fold=  3 / 5\n",
      "selection=  importance\n",
      "selected 33 feats using importance\n",
      " iter=  20 / 200  train rmse=  0.6617671086624839  val rmse=  0.6581949173275397\n",
      " iter=  40 / 200  train rmse=  0.6110610267661704  val rmse=  0.607415264470463\n",
      " iter=  60 / 200  train rmse=  0.5899757484479043  val rmse=  0.5869680872144243\n",
      " iter=  80 / 200  train rmse=  0.5799020768697576  val rmse=  0.5774607706390901\n",
      " iter=  100 / 200  train rmse=  0.5743837123897728  val rmse=  0.5723495455121813\n",
      " iter=  120 / 200  train rmse=  0.5706483428465827  val rmse=  0.5692459562223782\n",
      " iter=  140 / 200  train rmse=  0.5679689525509654  val rmse=  0.567175859884818\n",
      " iter=  160 / 200  train rmse=  0.5658639615575377  val rmse=  0.5655786368622968\n",
      " iter=  180 / 200  train rmse=  0.5640960023332205  val rmse=  0.5642691771528385\n",
      " iter=  200 / 200  train rmse=  0.5626030089160228  val rmse=  0.5633155873216756\n",
      "fold metrics= rmse=  0.5633155873216756  mae=  0.43220687430046284  mape=  23.008456099315403  r2=  0.49001015949642\n",
      "fold=  4 / 5\n",
      "selection=  importance\n",
      "selected 33 feats using importance\n",
      " iter=  20 / 200  train rmse=  0.6597698385287438  val rmse=  0.6667607572164481\n",
      " iter=  40 / 200  train rmse=  0.6092754196873654  val rmse=  0.6170834345825992\n",
      " iter=  60 / 200  train rmse=  0.5881293926135723  val rmse=  0.596699841269947\n",
      " iter=  80 / 200  train rmse=  0.5781673391030694  val rmse=  0.5875293838335444\n",
      " iter=  100 / 200  train rmse=  0.5730650720842013  val rmse=  0.5828684412461727\n",
      " iter=  120 / 200  train rmse=  0.5696583547835046  val rmse=  0.5798538673910132\n",
      " iter=  140 / 200  train rmse=  0.566991737721663  val rmse=  0.5776683506254245\n",
      " iter=  160 / 200  train rmse=  0.5648200042015619  val rmse=  0.5759455933990707\n",
      " iter=  180 / 200  train rmse=  0.5630117880155361  val rmse=  0.5746927165519343\n",
      " iter=  200 / 200  train rmse=  0.5614015016600589  val rmse=  0.5736092193298248\n",
      "fold metrics= rmse=  0.5736092193298248  mae=  0.4415482548019229  mape=  23.62406445830294  r2=  0.47814861167699885\n",
      "fold=  5 / 5\n",
      "selection=  importance\n",
      "selected 33 feats using importance\n",
      " iter=  20 / 200  train rmse=  0.6613549709024407  val rmse=  0.6597061907760374\n",
      " iter=  40 / 200  train rmse=  0.6109136052018233  val rmse=  0.6098061084837504\n",
      " iter=  60 / 200  train rmse=  0.5895579715496942  val rmse=  0.5885581649374582\n",
      " iter=  80 / 200  train rmse=  0.5796380536029442  val rmse=  0.5790233202791988\n",
      " iter=  100 / 200  train rmse=  0.5742227218734308  val rmse=  0.574014844124411\n",
      " iter=  120 / 200  train rmse=  0.5707033751391293  val rmse=  0.5708159348272958\n",
      " iter=  140 / 200  train rmse=  0.5680396945636247  val rmse=  0.5685443792438207\n",
      " iter=  160 / 200  train rmse=  0.5657753014721609  val rmse=  0.5665121775311921\n",
      " iter=  180 / 200  train rmse=  0.5639221015830118  val rmse=  0.5650084131178597\n",
      " iter=  200 / 200  train rmse=  0.5623251785505871  val rmse=  0.5637315545103386\n",
      "fold metrics= rmse=  0.5637315545103386  mae=  0.4344765719739171  mape=  22.93552744193495  r2=  0.4889839507157857\n",
      "cv avg= rmse=  0.566886146637069  mae=  0.4370010543081748  mape=  23.271600715084027  r2=  0.48530202969782604\n",
      "model=  GBDT_LogPrice  params=  {'n_estimators': 200, 'learning_rate': 0.03, 'max_leaves': 12, 'min_samples': 20, 'early_stopping': True, 'early_stopping_rounds': 10, 'use_sample_weights': False}\n",
      "fold=  1 / 5\n",
      " iter=  20 / 200  train rmse=  0.6605427949073698  val rmse=  0.6625142427356777\n",
      " iter=  40 / 200  train rmse=  0.6086951003626094  val rmse=  0.6121066841763012\n",
      " iter=  60 / 200  train rmse=  0.586082201961371  val rmse=  0.5898251355323944\n",
      " iter=  80 / 200  train rmse=  0.5745300272500018  val rmse=  0.5785672576011327\n",
      " iter=  100 / 200  train rmse=  0.5681449110847868  val rmse=  0.5726155365275211\n",
      " iter=  120 / 200  train rmse=  0.5636491308444909  val rmse=  0.568569902929112\n",
      " iter=  140 / 200  train rmse=  0.5600648165992572  val rmse=  0.5651989331149159\n",
      " iter=  160 / 200  train rmse=  0.5570974009929526  val rmse=  0.5624416165831881\n",
      " iter=  180 / 200  train rmse=  0.5547545004219974  val rmse=  0.5602882572860127\n",
      " iter=  200 / 200  train rmse=  0.5527186065764452  val rmse=  0.5586016571393082\n",
      "fold metrics= rmse=  0.5586016571393082  mae=  0.43100965368464295  mape=  22.99389250779404  r2=  0.4971179474778039\n",
      "fold=  2 / 5\n",
      " iter=  20 / 200  train rmse=  0.6606805213314227  val rmse=  0.6612198001963809\n",
      " iter=  40 / 200  train rmse=  0.6089062145878337  val rmse=  0.6094416983215492\n",
      " iter=  60 / 200  train rmse=  0.5863079726122749  val rmse=  0.5870886015949407\n",
      " iter=  80 / 200  train rmse=  0.5750736746984982  val rmse=  0.5761955014566696\n",
      " iter=  100 / 200  train rmse=  0.5687263335753879  val rmse=  0.570348850620949\n",
      " iter=  120 / 200  train rmse=  0.5640089759855943  val rmse=  0.5663018623560807\n",
      " iter=  140 / 200  train rmse=  0.5603721880532081  val rmse=  0.5632210427578483\n",
      " iter=  160 / 200  train rmse=  0.5574158163491268  val rmse=  0.5606860324571773\n",
      " iter=  180 / 200  train rmse=  0.5548191344663593  val rmse=  0.5585215884293517\n",
      " iter=  200 / 200  train rmse=  0.5528948315096032  val rmse=  0.5570474447793948\n",
      "fold metrics= rmse=  0.5570474447793948  mae=  0.4300477146720346  mape=  23.130298674516162  r2=  0.5049527817250893\n",
      "fold=  3 / 5\n",
      " iter=  20 / 200  train rmse=  0.6613316435518906  val rmse=  0.6577762252812372\n",
      " iter=  40 / 200  train rmse=  0.6093607590032608  val rmse=  0.6059838368994195\n",
      " iter=  60 / 200  train rmse=  0.5865069391121976  val rmse=  0.5843262487925371\n",
      " iter=  80 / 200  train rmse=  0.5753452072968549  val rmse=  0.5740409581064141\n",
      " iter=  100 / 200  train rmse=  0.568949253678711  val rmse=  0.5685300912081501\n",
      " iter=  120 / 200  train rmse=  0.5645405419813962  val rmse=  0.564879822872293\n",
      " iter=  140 / 200  train rmse=  0.5610342727443821  val rmse=  0.5620064291077513\n",
      " iter=  160 / 200  train rmse=  0.5582061934602254  val rmse=  0.5597607986289806\n",
      " iter=  180 / 200  train rmse=  0.5557274657961585  val rmse=  0.557908994128331\n",
      " iter=  200 / 200  train rmse=  0.5538678458172149  val rmse=  0.556669639868334\n",
      "fold metrics= rmse=  0.556669639868334  mae=  0.4262716646959683  mape=  22.67031822607349  r2=  0.501972803667319\n",
      "fold=  4 / 5\n",
      " iter=  20 / 200  train rmse=  0.6593949599150628  val rmse=  0.6663687341218227\n",
      " iter=  40 / 200  train rmse=  0.6076604839263444  val rmse=  0.6156032904369156\n",
      " iter=  60 / 200  train rmse=  0.5849398215382312  val rmse=  0.5934751331776936\n",
      " iter=  80 / 200  train rmse=  0.5734131046188345  val rmse=  0.5828811307780326\n",
      " iter=  100 / 200  train rmse=  0.5671566869275572  val rmse=  0.5771239243242373\n",
      " iter=  120 / 200  train rmse=  0.5622738155758668  val rmse=  0.572976092274708\n",
      " iter=  140 / 200  train rmse=  0.5585070881747044  val rmse=  0.5700306988007144\n",
      " iter=  160 / 200  train rmse=  0.555650680649965  val rmse=  0.5677796057908455\n",
      " iter=  180 / 200  train rmse=  0.5534227493405091  val rmse=  0.5661792906133447\n",
      " iter=  200 / 200  train rmse=  0.551400420922733  val rmse=  0.5646314385884479\n",
      "fold metrics= rmse=  0.5646314385884479  mae=  0.43309818757282714  mape=  23.281542976123774  r2=  0.49435617348987726\n",
      "fold=  5 / 5\n",
      " iter=  20 / 200  train rmse=  0.661055006450739  val rmse=  0.6594973046328149\n",
      " iter=  40 / 200  train rmse=  0.609649531104146  val rmse=  0.6084600849091338\n",
      " iter=  60 / 200  train rmse=  0.5871231366883604  val rmse=  0.5859836767613049\n",
      " iter=  80 / 200  train rmse=  0.5758753322447467  val rmse=  0.5746195688560592\n",
      " iter=  100 / 200  train rmse=  0.569649095225641  val rmse=  0.56837619385778\n",
      " iter=  120 / 200  train rmse=  0.5653436191512325  val rmse=  0.5641776723961678\n",
      " iter=  140 / 200  train rmse=  0.5617340595113312  val rmse=  0.5605533833013386\n",
      " iter=  160 / 200  train rmse=  0.5587368347371351  val rmse=  0.5576875197525302\n",
      " iter=  180 / 200  train rmse=  0.5564353078762682  val rmse=  0.5555680574752639\n",
      " iter=  200 / 200  train rmse=  0.5545598894433643  val rmse=  0.5540028570152005\n",
      "fold metrics= rmse=  0.5540028570152005  mae=  0.4252766477886686  mape=  22.55837025519417  r2=  0.5064696567025937\n",
      "cv avg= rmse=  0.5581906074781371  mae=  0.4291407736828283  mape=  22.926884527940324  r2=  0.5009738726125367\n",
      "final training on full data now for  GBDT_LogPrice\n",
      " iter=  20 / 200  train rmse=  0.6614282484629386  val rmse=  0.6563780204021998\n",
      " iter=  40 / 200  train rmse=  0.609612226487355  val rmse=  0.6060941057450759\n",
      " iter=  60 / 200  train rmse=  0.5867634326762926  val rmse=  0.5845798442585911\n",
      " iter=  80 / 200  train rmse=  0.5754173007863771  val rmse=  0.5738707200229849\n",
      " iter=  100 / 200  train rmse=  0.5690547185197959  val rmse=  0.5681133524579074\n",
      " iter=  120 / 200  train rmse=  0.5644715096200551  val rmse=  0.5640753791578463\n",
      " iter=  140 / 200  train rmse=  0.5608686645879178  val rmse=  0.5608539736962349\n",
      " iter=  160 / 200  train rmse=  0.5579443642787538  val rmse=  0.5584442121481352\n",
      " iter=  180 / 200  train rmse=  0.5557438693861237  val rmse=  0.5566926806370223\n",
      " iter=  200 / 200  train rmse=  0.5536753001660059  val rmse=  0.5551533116081763\n",
      "test metrics orig scale=  {'RMSE (orig)': 9.112990990567347, 'MAE (orig)': 4.8051337928443365, 'MAPE (%) (orig)': 58.72141924552204}\n",
      "bucket acc=  39.586853017969595  plusminus1=  87.28305943787437\n",
      "saved final model=  models\\gbdt_logprice_final.joblib\n",
      "7) Feature Importance\n",
      "====================================================\n",
      "best ridge=  MLR_Ridge_LogPrice  mae=  0.45210173099159656\n",
      "best GBDT=  GBDT_LogPrice  mae=  0.42486699925497884\n",
      "----- ridge feature importance maybe -----\n",
      "Top 20 feats by absolute coef:\n",
      "                    Feature  Coefficient\n",
      "66         is_free_or_cheap    -0.319487\n",
      "57        reviews_total_log     0.093291\n",
      "60          stsp_owners_log    -0.069351\n",
      "43         categories_count     0.067041\n",
      "63      score_x_log_reviews     0.064800\n",
      "18          tags_has_casual    -0.062143\n",
      "27      tags_has_story_rich     0.049878\n",
      "62        hltb_complete_log     0.045209\n",
      "22     tags_has_multiplayer     0.041527\n",
      "32         genres_has_indie    -0.040808\n",
      "61          hltb_single_log     0.039178\n",
      "14           tags_has_indie    -0.038681\n",
      "40  genres_has_early_access     0.038414\n",
      "26          tags_has_puzzle    -0.036042\n",
      "25      tags_has_open_world     0.030903\n",
      "58         achievements_log     0.029329\n",
      "23    tags_has_early_access     0.029288\n",
      "12       has_indie_keywords    -0.028823\n",
      "1           is_highly_rated    -0.027456\n",
      "20      tags_has_simulation     0.027253\n",
      "saved ridge feats png\n",
      "saved cat importance\n",
      "by cat:\n",
      "Genre :  0.18927954816186704  ->  30.751033831878033 %\n",
      "Engagement :  0.12730799289743397  ->  20.682912838050513 %\n",
      "Publisher :  0.1266550645837437  ->  20.576835763907724 %\n",
      "GameContent :  0.1081554902868004  ->  17.57132861532687 %\n",
      "Quality :  0.06412445591896647  ->  10.417888950836868 %\n",
      "Age :  0.0  ->  0.0 %\n",
      "----- GBDT feature importance maybe -----\n",
      "we have 200 trees checking feature_importances\n",
      "Top 20 feats for GBDT:\n",
      "                     Feature  Avg_Importance\n",
      "66          is_free_or_cheap        0.190041\n",
      "57         reviews_total_log        0.073312\n",
      "58          achievements_log        0.064553\n",
      "60           stsp_owners_log        0.050357\n",
      "43          categories_count        0.048198\n",
      "62         hltb_complete_log        0.032894\n",
      "63       score_x_log_reviews        0.029206\n",
      "55      publishers_log_games        0.028934\n",
      "61           hltb_single_log        0.028400\n",
      "27       tags_has_story_rich        0.024958\n",
      "10                tags_count        0.024286\n",
      "0   reviews_score_fancy_norm        0.023996\n",
      "52     publishers_game_count        0.023721\n",
      "26           tags_has_puzzle        0.023009\n",
      "40   genres_has_early_access        0.021039\n",
      "18           tags_has_casual        0.019074\n",
      "25       tags_has_open_world        0.017344\n",
      "32          genres_has_indie        0.017005\n",
      "22      tags_has_multiplayer        0.016993\n",
      "20       tags_has_simulation        0.016783\n",
      "saved GDBT feats png\n",
      "importance stability top10:\n",
      "                 Feature  Avg_Importance  Importance_StdDev  Importance_CV\n",
      "66      is_free_or_cheap        0.190041           0.042510       1.771513\n",
      "57     reviews_total_log        0.073312           0.000000       0.000000\n",
      "58      achievements_log        0.064553           0.039917       2.586600\n",
      "60       stsp_owners_log        0.050357           0.000000       0.000000\n",
      "43      categories_count        0.048198           0.043727       2.691711\n",
      "62     hltb_complete_log        0.032894           0.005733       9.950588\n",
      "63   score_x_log_reviews        0.029206           0.000000       0.000000\n",
      "55  publishers_log_games        0.028934           0.035415       2.436755\n",
      "61       hltb_single_log        0.028400           0.018936       4.034404\n",
      "27   tags_has_story_rich        0.024958           0.000000       0.000000\n",
      "8) Model Load & Inference example\n",
      "======================================\n",
      "Best model is=  GBDT_LogPrice\n",
      "Loaded model from disk\n",
      "Sample predictions with loaded model:\n",
      "Game Title                            ActualPrice   PredPrice     AbsErr\n",
      "---------------------------------------------------------------------\n",
      "Chubby Bear Smash                        7.990000000000002          5.0         2.99\n",
      "Telepath Tactics Liberated               24.989999999999995        12.18        12.81\n",
      "Neckbeards: Basement Arena                       3.99         7.44         3.45\n",
      "PICK ME UP! - Rescue Rangers -           11.990000000000002         5.23         6.76\n",
      "Love Photo Studio/真爱照相馆                          2.99         4.75         1.76\n",
      "sample mae=  5.554922987590897  sample mape=  58.06949002336215  %\n",
      "in order to do inference with the saved model just do these steps basically:\n",
      "1) joblibload the file\n",
      "2) get initial_pred, trees, learning_rate, feature_names from it\n",
      "3) arrange input features similarly\n",
      "4) call GBpredict( X, init_pred, trees, lr ) to get predictions\n",
      "5) again do np.expm1 if it's log model\n",
      "9) Price Elasticity\n",
      "======================================================================\n",
      "Wrote elasticity distribution figure\n",
      "Wrote elasticity by Publisher figure\n",
      "Making discount recs\n",
      "saved discount distribution figure\n",
      "Price optimization with best model predictions\n",
      "sim results for price changes\n",
      "    price_adj_pct  avg_price  avg_revenue  avg_rev_change_pct\n",
      "0             -30   5.831128   767.787351           -4.806226\n",
      "1             -20   6.664146   803.246454           -1.333053\n",
      "2             -10   7.497165   824.506028            0.202983\n",
      "3              -5   7.913674   830.264812            0.306487\n",
      "4               0   8.330183   833.018299            0.000000\n",
      "5               5   8.746692   831.253754           -0.922848\n",
      "6              10   9.163201   823.135727           -2.712455\n",
      "7              15   9.579710   808.361671           -5.410094\n",
      "8              20   9.996220   786.629040           -9.057040\n",
      "9              25  10.412729   757.635286          -13.694567\n",
      "10             30  10.829238   721.077864          -19.363948\n",
      "saved simulation result figure\n",
      "optimal strategy maybe is=  -5.0  % price adjustment\n",
      "estimated revenue impact=  0.30648686404611863  %\n",
      "some sample game recs\n",
      "\n",
      "Game=  Total War: PHARAOH\n",
      "pred price=  8.83  elasticity=  -1.34  recommended discount=  68.4 %\n",
      "Price adjustment recommendation=  -5% to +5%\n",
      "Maybe keep current price\n",
      "Sale price around=  2.79\n",
      "\n",
      "Game=  Plague of Days\n",
      "pred price=  1.01  elasticity=  -1.51  recommended discount=  73.3 %\n",
      "Price adjustment recommendation=  -15% to -5%\n",
      "Try lowering price\n",
      "Sale price around=  0.27\n",
      "\n",
      "Game=  VCB: Why City 4k\n",
      "pred price=  9.03  elasticity=  -1.25  recommended discount=  54.1 %\n",
      "Price adjustment recommendation=  -5% to +5%\n",
      "Maybe keep current price\n",
      "Sale price around=  4.14\n",
      "\n",
      "Game=  FOTONICA\n",
      "pred price=  8.97  elasticity=  -0.78  recommended discount=  43.3 %\n",
      "Price adjustment recommendation=  +5% to +15%\n",
      "Premium pricing recommended\n",
      "Sale price around=  5.09\n",
      "\n",
      "Game=  Sacraboar\n",
      "pred price=  7.52  elasticity=  -1.11  recommended discount=  63.9 %\n",
      "Price adjustment recommendation=  -5% to +5%\n",
      "Maybe keep current price\n",
      "Sale price around=  2.72\n",
      "10) Conclusion\n",
      "==========================================\n",
      "comparing final model performance:\n",
      "direct price models:\n",
      "                     RMSE       MAE    MAPE (%)        R2  Bucket Acc  Bucket Acc (±1)\n",
      "MLR_Ridge_Price  9.289197  5.513528  101.188876  0.267259   29.657503        79.219782\n",
      "GBDT_Price       8.868865  6.127612  126.524784  0.332071   23.667639        65.681155\n",
      "log price models:\n",
      "                        RMSE  RMSE (orig)       MAE   MAPE (%)        R2  Bucket Acc  Bucket Acc (±1)\n",
      "GBDT_LogPrice       0.551472     9.112991  0.424867  22.871380  0.512921   39.586853        87.283059\n",
      "MLR_Ridge_LogPrice  0.573728     9.457147  0.452102  25.160512  0.472813   37.935801        86.169559\n",
      "Best model=  GBDT_LogPrice\n",
      "MAE=  0.42486699925497884\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"6) Training Models\")\n",
    "print(\"===========================================\")\n",
    "\n",
    "if not alldata.empty and 'target_price' in alldata.columns and alldata['target_price'].notna().all():\n",
    "    if 'price_bucket_id' not in alldata.columns:\n",
    "        pbb = [0,4.99,9.99,14.99,19.99,29.99,39.99,59.99,float('inf')]\n",
    "        alldata['price_bucket_id'] = pd.cut(alldata['target_price'], bins=pbb, labels=False, right=True)\n",
    "\n",
    "    keepcols = [c for c in alldata.columns if c not in ['target_price','price_bucket','price_bucket_id','log_price','hours_per_dollar','is_high_value','content_price_ratio','content_price_ratio_log','optimal_price_heuristic','recommended_discount_heuristic','price_elasticity']]\n",
    "    print(\"Making base feats and targets\")\n",
    "\n",
    "    yreg = alldata['target_price'].copy()\n",
    "    ylog = alldata['log_price'].copy()\n",
    "    ybuck = alldata['price_bucket_id'].copy()\n",
    "\n",
    "    Xbase = alldata[keepcols].copy()\n",
    "\n",
    "    print(\"split train test 80 20 with stratify on bucket\")\n",
    "    X_train_base, X_test_base, ytrainreg, ytestreg = train_test_split(\n",
    "        Xbase, yreg, test_size=0.2, random_state=seed, stratify=alldata['price_bucket_id']\n",
    "    )\n",
    "    ytrainlog = ylog.loc[ytrainreg.index]\n",
    "    ytestlog = ylog.loc[ytestreg.index]\n",
    "    ytrainbucket = ybuck.loc[ytrainreg.index]\n",
    "    ytestbucket = ybuck.loc[ytestreg.index]\n",
    "\n",
    "    print(\"train size= \", X_train_base.shape[0], \" test size= \", X_test_base.shape[0])\n",
    "\n",
    "    print(\"feature engineering separately for train test\")\n",
    "    xtraineng, xtesteng, englist = make_features(X_train_base, X_test_base)\n",
    "    print(\"got \" + str(len(englist)) + \" eng feats in total\")\n",
    "\n",
    "    nf = 5\n",
    "    cv = StratifiedKFold(n_splits=nf, shuffle=True, random_state=seed)\n",
    "    print(\"trying \" + str(nf) + \"-fold strat cv now\")\n",
    "\n",
    "    allresults = {}\n",
    "\n",
    "    def evalmodel(yt, yp, name, part=\"test\"):\n",
    "        mm = {}\n",
    "        mm['RMSE'] = np.sqrt(mean_squared_error(yt, yp))\n",
    "        mm['MAE'] = mean_absolute_error(yt, yp)\n",
    "        mm['MAPE (%)'] = mean_absolute_percentage_error(yt, yp)*100\n",
    "        mm['R2'] = r2_score(yt, yp)\n",
    "        if name not in allresults:\n",
    "            allresults[name] = {}\n",
    "        if part not in allresults[name]:\n",
    "            allresults[name][part] = {}\n",
    "        for mk,mv in mm.items():\n",
    "            if mk not in allresults[name][part]:\n",
    "                allresults[name][part][mk] = []\n",
    "            allresults[name][part][mk].append(mv)\n",
    "        return mm\n",
    "\n",
    "    def priceToBucket(prices):\n",
    "        pbb = [0,4.99,9.99,14.99,19.99,29.99,39.99,59.99,float('inf')]\n",
    "        return pd.cut(prices, bins=pbb, labels=False, right=True)\n",
    "\n",
    "    def bucketAcc(yt, yp, tol=0):\n",
    "        predb = priceToBucket(yp)\n",
    "        if tol==0:\n",
    "            return np.mean(predb==yt)*100\n",
    "        else:\n",
    "            return np.mean(np.abs(predb-yt)<=tol)*100\n",
    "\n",
    "    print(\"feature selection approach maybe\")\n",
    "    numfeats = [f for f in englist if pd.api.types.is_numeric_dtype(xtraineng[f])]\n",
    "\n",
    "    xtrainnum = xtraineng[numfeats].copy()\n",
    "    xtestnum = xtesteng[numfeats].copy()\n",
    "    xtrainnum.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    xtestnum.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "    xtrainnum.fillna(0, inplace=True)\n",
    "    xtestnum.fillna(0, inplace=True)\n",
    "\n",
    "    selmethods = ['importance','none']\n",
    "\n",
    "    # ridge\n",
    "    print(\"train ridge models\")\n",
    "    ridgepars = {\n",
    "        'regular':{\n",
    "            'lr':0.005,\n",
    "            'iterations':1000,\n",
    "            'lambda':0.5,\n",
    "            'early_stopping':True\n",
    "        },\n",
    "        'log':{\n",
    "            'lr':0.01,\n",
    "            'iterations':800,\n",
    "            'lambda':0.1,\n",
    "            'early_stopping':True\n",
    "        }\n",
    "    }\n",
    "    final_ridge = {}\n",
    "\n",
    "    for ttype in ['regular','log']:\n",
    "        tname = 'Price' if ttype=='regular' else 'LogPrice'\n",
    "        print(\"training ridge for: \", tname)\n",
    "        for selm in selmethods:\n",
    "            mname = \"MLR_Ridge_\"+tname\n",
    "            if selm!='none':\n",
    "                mname+=\"_\"+selm\n",
    "            pars = ridgepars[ttype]\n",
    "            print(\"model= \", mname,\" params= \", pars)\n",
    "            foldpreds=[]\n",
    "            foldy=[]\n",
    "            foldms = []\n",
    "            ytrainThis = ytrainreg if ttype=='regular' else ytrainlog\n",
    "            ytestThis = ytestreg if ttype=='regular' else ytestlog\n",
    "            for fold,(trix,valix) in enumerate(cv.split(xtrainnum, ytrainbucket)):\n",
    "                print(\"fold= \", fold+1, \" / \", nf)\n",
    "                Xftrain, Xfval = xtrainnum.iloc[trix], xtrainnum.iloc[valix]\n",
    "                yftrain, yfval = ytrainThis.iloc[trix], ytrainThis.iloc[valix]\n",
    "                if selm!='none':\n",
    "                    print(\"doing \", selm, \" selection\")\n",
    "                    Xft_sel, idxsel = select_features(Xftrain.values, yftrain.values, method=selm, n_features=min(100, Xftrain.shape[1]//2))\n",
    "                    Xfv_sel = Xfval.values[:,idxsel]\n",
    "                    featnames = Xftrain.columns[idxsel].tolist()\n",
    "                else:\n",
    "                    Xft_sel = Xftrain.values\n",
    "                    Xfv_sel = Xfval.values\n",
    "                    featnames = Xftrain.columns.tolist()\n",
    "\n",
    "                stime = time.time()\n",
    "                W, ch, mmu, mss = RidgeFit(\n",
    "                    Xft_sel, yftrain.values,\n",
    "                    lr=pars['lr'],\n",
    "                    numiter=pars['iterations'],\n",
    "                    lam=pars['lambda'],\n",
    "                    early_stop=pars['early_stopping'],\n",
    "                    verbose=True,\n",
    "                    pstep=200\n",
    "                )\n",
    "                ttime = time.time()-stime\n",
    "                yvpred = RidgePredict(Xfv_sel, W, mmu, mss)\n",
    "                foldpreds.append(yvpred)\n",
    "                foldy.append(yfval.values)\n",
    "                ms = evalmodel(yfval.values, yvpred, mname, part=\"fold_\"+str(fold+1))\n",
    "                ms['Training Time']=ttime\n",
    "                foldms.append(ms)\n",
    "                print(\"fold metrics= rmse= \", ms['RMSE'], \" mae= \", ms['MAE'], \" mape= \", ms['MAPE (%)'], \" r2= \", ms['R2'])\n",
    "            avgrmse = np.mean([m['RMSE'] for m in foldms])\n",
    "            avgmae = np.mean([m['MAE'] for m in foldms])\n",
    "            avgmape = np.mean([m['MAPE (%)'] for m in foldms])\n",
    "            avgr2 = np.mean([m['R2'] for m in foldms])\n",
    "            print(\"cv average= rmse= \", avgrmse, \" mae= \", avgmae, \" mape= \", avgmape, \" r2= \", avgr2)\n",
    "\n",
    "            if selm=='none':\n",
    "                print(\"train final on full training set no selection\")\n",
    "                if selm!='none':\n",
    "                    pass\n",
    "                else:\n",
    "                    Xtrain_sel = xtrainnum.values\n",
    "                    Xtest_sel = xtestnum.values\n",
    "                    final_feats = xtrainnum.columns.tolist()\n",
    "\n",
    "                stime2 = time.time()\n",
    "                W, ch, mmu, mss = RidgeFit(\n",
    "                    Xtrain_sel, ytrainThis.values,\n",
    "                    lr=pars['lr'],\n",
    "                    numiter=pars['iterations'],\n",
    "                    lam=pars['lambda'],\n",
    "                    early_stop=pars['early_stopping'],\n",
    "                    verbose=True,\n",
    "                    pstep=200\n",
    "                )\n",
    "                fttime2 = time.time()-stime2\n",
    "                ytpred = RidgePredict(Xtest_sel, W, mmu, mss)\n",
    "                testms = evalmodel(ytestThis.values, ytpred, mname, part=\"test\")\n",
    "                testms['Training Time']=fttime2\n",
    "\n",
    "                if ttype=='log':\n",
    "                    ypreorig = np.expm1(ytpred)\n",
    "                    ytestorig = np.expm1(ytestThis.values)\n",
    "                    om = {}\n",
    "                    om['RMSE (orig)'] = np.sqrt(mean_squared_error(ytestorig, ypreorig))\n",
    "                    om['MAE (orig)'] = mean_absolute_error(ytestorig, ypreorig)\n",
    "                    om['MAPE (%) (orig)'] = mean_absolute_percentage_error(ytestorig, ypreorig)*100\n",
    "                    for k,v in om.items():\n",
    "                        if k not in allresults[mname][\"test\"]:\n",
    "                            allresults[mname][\"test\"][k]=[]\n",
    "                        allresults[mname][\"test\"][k].append(v)\n",
    "                    print(\"test metrics on original scale= \", om)\n",
    "\n",
    "                if ttype=='log':\n",
    "                    pred4b = np.expm1(ytpred)\n",
    "                else:\n",
    "                    pred4b = ytpred\n",
    "                bacc = bucketAcc(ytestbucket.values, pred4b, tol=0)\n",
    "                bacc1 = bucketAcc(ytestbucket.values, pred4b, tol=1)\n",
    "                if 'Price Bucket Accuracy' not in allresults[mname][\"test\"]:\n",
    "                    allresults[mname][\"test\"]['Price Bucket Accuracy']=[]\n",
    "                allresults[mname][\"test\"]['Price Bucket Accuracy'].append(bacc)\n",
    "                if 'Price Bucket Accuracy (±1)' not in allresults[mname][\"test\"]:\n",
    "                    allresults[mname][\"test\"]['Price Bucket Accuracy (±1)']=[]\n",
    "                allresults[mname][\"test\"]['Price Bucket Accuracy (±1)'].append(bacc1)\n",
    "                print(\"bucket acc= \", bacc, \" bucket acc ±1= \", bacc1)\n",
    "\n",
    "                fm = {\n",
    "                    'weights':W,\n",
    "                    'means':mmu,\n",
    "                    'stds':mss,\n",
    "                    'feature_names':final_feats,\n",
    "                    'params':pars,\n",
    "                    'metrics':testms,\n",
    "                    'is_log_model':(ttype=='log')\n",
    "                }\n",
    "                fnam = mname.lower()+\"_final.joblib\"\n",
    "                pathf = os.path.join(modelfolder, fnam)\n",
    "                joblib.dump(fm, pathf)\n",
    "                print(\"saved final model= \", pathf)\n",
    "                final_ridge[mname]=fm\n",
    "\n",
    "    # GBDT\n",
    "    print(\"Training gradient boosting from scratch\")\n",
    "    gpar = {\n",
    "        'regular':{\n",
    "            'n_estimators':200,\n",
    "            'learning_rate':0.05,\n",
    "            'max_leaves':12,\n",
    "            'min_samples':20,\n",
    "            'early_stopping':True,\n",
    "            'early_stopping_rounds':10,\n",
    "            'use_sample_weights':True\n",
    "        },\n",
    "        'log':{\n",
    "            'n_estimators':200,\n",
    "            'learning_rate':0.03,\n",
    "            'max_leaves':12,\n",
    "            'min_samples':20,\n",
    "            'early_stopping':True,\n",
    "            'early_stopping_rounds':10,\n",
    "            'use_sample_weights':False\n",
    "        }\n",
    "    }\n",
    "    final_gb = {}\n",
    "\n",
    "    for ttype in ['regular','log']:\n",
    "        tname = 'Price' if ttype=='regular' else 'LogPrice'\n",
    "        print(\"Training GBDT for= \", tname)\n",
    "        for selm in selmethods:\n",
    "            mname = \"GBDT_\"+tname\n",
    "            if selm!='none':\n",
    "                mname+=\"_\"+selm\n",
    "            pp = gpar[ttype]\n",
    "            print(\"model= \", mname, \" params= \", pp)\n",
    "            foldpreds=[]\n",
    "            foldy=[]\n",
    "            foldms=[]\n",
    "            ytrainThis = ytrainreg if ttype=='regular' else ytrainlog\n",
    "            ytestThis = ytestreg if ttype=='regular' else ytestlog\n",
    "            for fold,(trix,valix) in enumerate(cv.split(xtrainnum, ytrainbucket)):\n",
    "                print(\"fold= \", fold+1, \"/\", nf)\n",
    "                Xftrain, Xfval = xtrainnum.iloc[trix], xtrainnum.iloc[valix]\n",
    "                yftrain, yfval = ytrainThis.iloc[trix], ytrainThis.iloc[valix]\n",
    "                if selm!='none':\n",
    "                    print(\"selection= \", selm)\n",
    "                    Xft_sel, idxsel = select_features(Xftrain.values, yftrain.values, method=selm, n_features=min(100, Xftrain.shape[1]//2))\n",
    "                    Xfv_sel = Xfval.values[:,idxsel]\n",
    "                    fnames = Xftrain.columns[idxsel].tolist()\n",
    "                else:\n",
    "                    Xft_sel = Xftrain.values\n",
    "                    Xfv_sel = Xfval.values\n",
    "                    fnames = Xftrain.columns.tolist()\n",
    "                startt = time.time()\n",
    "                ip, trees, lrate = GBfit(\n",
    "                    Xft_sel, yftrain.values,\n",
    "                    n=pp['n_estimators'],\n",
    "                    lr=pp['learning_rate'],\n",
    "                    maxleaf=pp['max_leaves'],\n",
    "                    minleaf=pp['min_samples'],\n",
    "                    verbose=True,\n",
    "                    pstep=20,\n",
    "                    Xv=Xfv_sel,\n",
    "                    yv=yfval.values,\n",
    "                    esr=pp['early_stopping_rounds'],\n",
    "                    usew=pp['use_sample_weights']\n",
    "                )\n",
    "                ttt = time.time()-startt\n",
    "                yvpred = GBpredict(Xfv_sel, ip, trees, lrate)\n",
    "                foldpreds.append(yvpred)\n",
    "                foldy.append(yfval.values)\n",
    "                ms = evalmodel(yfval.values, yvpred, mname, part=\"fold_\"+str(fold+1))\n",
    "                ms['Training Time']=ttt\n",
    "                foldms.append(ms)\n",
    "                print(\"fold metrics= rmse= \", ms['RMSE'], \" mae= \", ms['MAE'], \" mape= \", ms['MAPE (%)'], \" r2= \", ms['R2'])\n",
    "            avgrmse = np.mean([m['RMSE'] for m in foldms])\n",
    "            avgmae = np.mean([m['MAE'] for m in foldms])\n",
    "            avgmape = np.mean([m['MAPE (%)'] for m in foldms])\n",
    "            avgr2 = np.mean([m['R2'] for m in foldms])\n",
    "            print(\"cv avg= rmse= \", avgrmse, \" mae= \", avgmae, \" mape= \", avgmape, \" r2= \", avgr2)\n",
    "\n",
    "            if selm=='none':\n",
    "                print(\"final training on full data now for \", mname)\n",
    "                if selm!='none':\n",
    "                    pass\n",
    "                else:\n",
    "                    Xtrain_sel = xtrainnum.values\n",
    "                    Xtest_sel = xtestnum.values\n",
    "                    finfnames = xtrainnum.columns.tolist()\n",
    "                valsize = 0.15\n",
    "                vix = np.random.choice(len(Xtrain_sel), int(len(Xtrain_sel)*valsize), replace=False)\n",
    "                masktr = np.ones(len(Xtrain_sel), dtype=bool)\n",
    "                masktr[vix]=False\n",
    "                Xtr_final = Xtrain_sel[masktr]\n",
    "                ytr_final = ytrainThis.values[masktr]\n",
    "                Xval_final = Xtrain_sel[~masktr]\n",
    "                yval_final = ytrainThis.values[~masktr]\n",
    "\n",
    "                st3 = time.time()\n",
    "                ip, trees, lrate = GBfit(\n",
    "                    Xtr_final, ytr_final,\n",
    "                    n=pp['n_estimators'],\n",
    "                    lr=pp['learning_rate'],\n",
    "                    maxleaf=pp['max_leaves'],\n",
    "                    minleaf=pp['min_samples'],\n",
    "                    verbose=True,\n",
    "                    pstep=20,\n",
    "                    Xv=Xval_final,\n",
    "                    yv=yval_final,\n",
    "                    esr=pp['early_stopping_rounds'],\n",
    "                    usew=pp['use_sample_weights']\n",
    "                )\n",
    "                ftdur3 = time.time()-st3\n",
    "                ytpred = GBpredict(Xtest_sel, ip, trees, lrate)\n",
    "                testms = evalmodel(ytestThis.values, ytpred, mname, part=\"test\")\n",
    "                testms['Training Time'] = ftdur3\n",
    "\n",
    "                if ttype=='log':\n",
    "                    ypreor = np.expm1(ytpred)\n",
    "                    ytestor = np.expm1(ytestThis.values)\n",
    "                    om = {}\n",
    "                    om['RMSE (orig)']=np.sqrt(mean_squared_error(ytestor, ypreor))\n",
    "                    om['MAE (orig)']=mean_absolute_error(ytestor, ypreor)\n",
    "                    om['MAPE (%) (orig)']=mean_absolute_percentage_error(ytestor, ypreor)*100\n",
    "                    for k,v in om.items():\n",
    "                        if k not in allresults[mname][\"test\"]:\n",
    "                            allresults[mname][\"test\"][k]=[]\n",
    "                        allresults[mname][\"test\"][k].append(v)\n",
    "                    print(\"test metrics orig scale= \", om)\n",
    "                if ttype=='log':\n",
    "                    p4b = np.expm1(ytpred)\n",
    "                else:\n",
    "                    p4b = ytpred\n",
    "                bacc = bucketAcc(ytestbucket.values, p4b, tol=0)\n",
    "                bacc1 = bucketAcc(ytestbucket.values, p4b, tol=1)\n",
    "                if 'Price Bucket Accuracy' not in allresults[mname][\"test\"]:\n",
    "                    allresults[mname][\"test\"]['Price Bucket Accuracy']=[]\n",
    "                allresults[mname][\"test\"]['Price Bucket Accuracy'].append(bacc)\n",
    "                if 'Price Bucket Accuracy (±1)' not in allresults[mname][\"test\"]:\n",
    "                    allresults[mname][\"test\"]['Price Bucket Accuracy (±1)']=[]\n",
    "                allresults[mname][\"test\"]['Price Bucket Accuracy (±1)'].append(bacc1)\n",
    "                print(\"bucket acc= \", bacc, \" plusminus1= \", bacc1)\n",
    "                fmodel = {\n",
    "                    'initial_pred':ip,\n",
    "                    'trees':trees,\n",
    "                    'learning_rate':lrate,\n",
    "                    'feature_names':finfnames,\n",
    "                    'params':pp,\n",
    "                    'metrics':testms,\n",
    "                    'is_log_model':(ttype=='log')\n",
    "                }\n",
    "                fname = mname.lower()+\"_final.joblib\"\n",
    "                ppth = os.path.join(modelfolder, fname)\n",
    "                joblib.dump(fmodel, ppth)\n",
    "                print(\"saved final model= \", ppth)\n",
    "                final_gb[mname]=fmodel\n",
    "\n",
    "    print(\"7) Feature Importance\")\n",
    "    print(\"====================================================\")\n",
    "\n",
    "    best_ridge_name = None\n",
    "    best_ridge_mae = float('inf')\n",
    "    for mn,mi in final_ridge.items():\n",
    "        if mi['is_log_model']:\n",
    "            mae = mi['metrics']['MAE (orig)'] if 'MAE (orig)' in mi['metrics'] else mi['metrics']['MAE']\n",
    "        else:\n",
    "            mae = mi['metrics']['MAE']\n",
    "        if mae<best_ridge_mae:\n",
    "            best_ridge_mae = mae\n",
    "            best_ridge_name = mn\n",
    "\n",
    "    best_gb_name = None\n",
    "    best_gb_mae = float('inf')\n",
    "    for mn,mi in final_gb.items():\n",
    "        if mi['is_log_model']:\n",
    "            mae = mi['metrics']['MAE (orig)'] if 'MAE (orig)' in mi['metrics'] else mi['metrics']['MAE']\n",
    "        else:\n",
    "            mae = mi['metrics']['MAE']\n",
    "        if mae<best_gb_mae:\n",
    "            best_gb_mae = mae\n",
    "            best_gb_name = mn\n",
    "\n",
    "    print(\"best ridge= \", best_ridge_name, \" mae= \", best_ridge_mae)\n",
    "    print(\"best GBDT= \", best_gb_name, \" mae= \", best_gb_mae)\n",
    "\n",
    "    print(\"----- ridge feature importance maybe -----\")\n",
    "    if best_ridge_name and best_ridge_name in final_ridge:\n",
    "        minf = final_ridge[best_ridge_name]\n",
    "        W = minf['weights']\n",
    "        fnames = minf['feature_names']\n",
    "        if len(W)>1 and len(fnames)==len(W)-1:\n",
    "            coefs = W[1:]\n",
    "            dfimp = pd.DataFrame({'Feature':fnames, 'Coefficient':coefs})\n",
    "            dfimp['Abs_Coefficient']=np.abs(dfimp['Coefficient'])\n",
    "            dfimp.sort_values('Abs_Coefficient', ascending=False, inplace=True)\n",
    "            print(\"Top 20 feats by absolute coef:\")\n",
    "            print(dfimp[['Feature','Coefficient']].head(20))\n",
    "\n",
    "            plt.figure(figsize=(12,10))\n",
    "            topf = dfimp.head(20).copy()\n",
    "            topf.sort_values('Coefficient', inplace=True)\n",
    "            colorz = ['red' if x<0 else 'green' for x in topf['Coefficient']]\n",
    "            plt.barh(topf['Feature'], topf['Coefficient'], color=colorz)\n",
    "            plt.title(\"Top 20 Ridge Features \" + best_ridge_name)\n",
    "            plt.axvline(x=0, color='black', linestyle='-', alpha=0.5)\n",
    "            plt.grid(True, axis='x')\n",
    "            plt.tight_layout()\n",
    "            plt.savefig('feature_importance_ridge_improved.png')\n",
    "            plt.close()\n",
    "            print(\"saved ridge feats png\")\n",
    "\n",
    "            cats = {\n",
    "                'Age':['age_','is_new_release'],\n",
    "                'Quality':['combined_score','quality_indicator','is_highly_rated'],\n",
    "                'Genre':['genres_has_','is_action_rpg'],\n",
    "                'Publisher':['publisher','developer'],\n",
    "                'Engagement':['achievements','has_achievements','reviews_'],\n",
    "                'GameContent':['hltb_','tags_count','genres_count']\n",
    "            }\n",
    "            catimp = {}\n",
    "            for cat, patlist in cats.items():\n",
    "                cfeats = []\n",
    "                for pat in patlist:\n",
    "                    cfeats.extend([f for f in dfimp['Feature'] if pat in f])\n",
    "                sabs = dfimp[dfimp['Feature'].isin(cfeats)]['Abs_Coefficient'].sum()\n",
    "                catimp[cat] = sabs\n",
    "            if catimp:\n",
    "                plt.figure(figsize=(10,6))\n",
    "                catdf = pd.DataFrame({'Category':list(catimp.keys()), 'Importance':list(catimp.values())})\n",
    "                catdf.sort_values('Importance', ascending=False, inplace=True)\n",
    "                plt.barh(catdf['Category'], catdf['Importance'])\n",
    "                plt.title(\"Feature Category Importance Ridge \" + best_ridge_name)\n",
    "                plt.grid(True, axis='x')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('feature_category_importance.png')\n",
    "                plt.close()\n",
    "                print(\"saved cat importance\")\n",
    "                print(\"by cat:\")\n",
    "                sst = sum(catimp.values())\n",
    "                for c,v in sorted(catimp.items(), key=lambda x: x[1], reverse=True):\n",
    "                    print(c,\": \", v, \" -> \", v/sst*100, \"%\")\n",
    "        else:\n",
    "            print(\"mismatch in shape\")\n",
    "    else:\n",
    "        print(\"no ridge model to do feat importance\")\n",
    "\n",
    "    print(\"----- GBDT feature importance maybe -----\")\n",
    "    if best_gb_name and best_gb_name in final_gb:\n",
    "        minf = final_gb[best_gb_name]\n",
    "        trees = minf['trees']\n",
    "        fnames = minf['feature_names']\n",
    "        if trees:\n",
    "            print(\"we have \" + str(len(trees)) + \" trees checking feature_importances\")\n",
    "            timports = []\n",
    "            for t in trees:\n",
    "                if hasattr(t,'feature_importances_'):\n",
    "                    timports.append(t.feature_importances_)\n",
    "            if timports and all(len(imp)==len(fnames) for imp in timports):\n",
    "                aimp = np.mean(timports, axis=0)\n",
    "                dfimp = pd.DataFrame({'Feature':fnames, 'Avg_Importance':aimp})\n",
    "                dfimp.sort_values('Avg_Importance', ascending=False, inplace=True)\n",
    "                print(\"Top 20 feats for GBDT:\")\n",
    "                print(dfimp.head(20))\n",
    "\n",
    "                plt.figure(figsize=(12,10))\n",
    "                topf = dfimp.head(20).copy()\n",
    "                plt.barh(topf['Feature'], topf['Avg_Importance'])\n",
    "                plt.title(\"Top 20 DGBT feats \" + best_gb_name)\n",
    "                plt.grid(True, axis='x')\n",
    "                plt.tight_layout()\n",
    "                plt.savefig('feature_importance_GBDT_improved.png')\n",
    "                plt.close()\n",
    "                print(\"saved GDBT feats png\")\n",
    "\n",
    "                if len(timports)>1:\n",
    "                    stdev = np.std(timports, axis=0)\n",
    "                    dfimp['Importance_StdDev'] = stdev\n",
    "                    dfimp['Importance_CV'] = stdev/(aimp+1e-10)\n",
    "                    print(\"importance stability top10:\")\n",
    "                    stable = dfimp.head(10)\n",
    "                    print(stable[['Feature','Avg_Importance','Importance_StdDev','Importance_CV']])\n",
    "            else:\n",
    "                print(\"Couldnt do average importances\")\n",
    "        else:\n",
    "            print(\"No trees found in best GBDT model?\")\n",
    "    else:\n",
    "        print(\"No GBDT model for feat importance\")\n",
    "\n",
    "    print(\"8) Model Load & Inference example\")\n",
    "    print(\"======================================\")\n",
    "    best_model_name = best_ridge_name if best_ridge_mae<=best_gb_mae else best_gb_name\n",
    "    best_model_type = \"Ridge\" if \"Ridge\" in best_model_name else \"GBDT\"\n",
    "    is_log = \"LogPrice\" in best_model_name\n",
    "    print(\"Best model is= \", best_model_name)\n",
    "\n",
    "    modfile = best_model_name.lower()+\"_final.joblib\"\n",
    "    mp = os.path.join(modelfolder, modfile)\n",
    "    try:\n",
    "        loaded_model = joblib.load(mp)\n",
    "        print(\"Loaded model from disk\")\n",
    "\n",
    "        def predict_with_model(dfxx, modelinfo, mtype=\"Ridge\"):\n",
    "            flz = modelinfo['feature_names']\n",
    "            for f in flz:\n",
    "                if f not in dfxx.columns:\n",
    "                    print(\"missing feature= \", f, \" filling with 0\")\n",
    "                    dfxx[f]=0\n",
    "            XX = dfxx[flz].values\n",
    "            if mtype==\"Ridge\":\n",
    "                W = modelinfo['weights']\n",
    "                mmu = modelinfo['means']\n",
    "                mss = modelinfo['stds']\n",
    "                p = RidgePredict(XX, W, mmu, mss)\n",
    "            else:\n",
    "                ipred = modelinfo['initial_pred']\n",
    "                tr = modelinfo['trees']\n",
    "                lr = modelinfo['learning_rate']\n",
    "                p = GBpredict(XX, ipred, tr, lr)\n",
    "            if modelinfo['is_log_model']:\n",
    "                p = np.expm1(p)\n",
    "            return p\n",
    "\n",
    "        nsample = min(5, len(xtesteng))\n",
    "        sidx = np.random.choice(len(xtesteng), nsample, replace=False)\n",
    "        sampleX = xtesteng.iloc[sidx]\n",
    "        if is_log:\n",
    "            samplT = np.expm1(ytestlog.iloc[sidx].values)\n",
    "        else:\n",
    "            samplT = ytestreg.iloc[sidx].values\n",
    "\n",
    "        preds = predict_with_model(sampleX, loaded_model, best_model_type)\n",
    "\n",
    "        print(\"Sample predictions with loaded model:\")\n",
    "        print(\"Game Title                            ActualPrice   PredPrice     AbsErr\")\n",
    "        print(\"---------------------------------------------------------------------\")\n",
    "        gtitle = []\n",
    "        if 'title' in alldata.columns:\n",
    "            gtitle = alldata.iloc[ytestreg.iloc[sidx].index]['title'].values\n",
    "        else:\n",
    "            gtitle = [\"Game \"+str(i+1) for i in range(nsample)]\n",
    "        for i,(tt,ac,pr) in enumerate(zip(gtitle, samplT, preds)):\n",
    "            shortt = (tt[:37]+'...') if len(tt)>40 else tt\n",
    "            print(shortt.ljust(40), str(ac).rjust(12), str(round(pr,2)).rjust(12), str(round(abs(ac-pr),2)).rjust(12))\n",
    "        smae = mean_absolute_error(samplT, preds)\n",
    "        smape = mean_absolute_percentage_error(samplT, preds)*100\n",
    "        print(\"sample mae= \", smae, \" sample mape= \", smape, \" %\")\n",
    "\n",
    "        print(\"in order to do inference with the saved model just do these steps basically:\")\n",
    "        if best_model_type==\"Ridge\":\n",
    "            print(\"1) joblibload the file\")\n",
    "            print(\"2) get weights, means, stds, feature_names from it\")\n",
    "            print(\"3) arrange input features in the same columns order\")\n",
    "            print(\"4) call RidgePredict( X, W, means, stds ) to get predictions\")\n",
    "            if is_log:\n",
    "                print(\"5) do np.expm1 on predictions to revert from log scale\")\n",
    "        else:\n",
    "            print(\"1) joblibload the file\")\n",
    "            print(\"2) get initial_pred, trees, learning_rate, feature_names from it\")\n",
    "            print(\"3) arrange input features similarly\")\n",
    "            print(\"4) call GBpredict( X, init_pred, trees, lr ) to get predictions\")\n",
    "            if is_log:\n",
    "                print(\"5) again do np.expm1 if it's log model\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"couldnt load or use model reason= \", e)\n",
    "\n",
    "    print(\"9) Price Elasticity\")\n",
    "    print(\"======================================================================\")\n",
    "\n",
    "    def est_elasticity(row, leak_free=True):\n",
    "        base = -1.5\n",
    "        sc = row.get('combined_score',0.5)\n",
    "        ag = row.get('age_years',1)\n",
    "        rv = row.get('reviews_total',0)\n",
    "        qi = row.get('quality_indicator',0.5)\n",
    "        aaa = row.get('publishers_is_aaa',0)\n",
    "        ihr = row.get('is_highly_rated',0)\n",
    "\n",
    "        if pd.notna(qi):\n",
    "            base += qi*0.8\n",
    "        if ihr:\n",
    "            base +=0.3\n",
    "        if aaa:\n",
    "            base +=0.35\n",
    "        if pd.notna(ag) and ag>0:\n",
    "            af = min(0.6, (ag/3)*0.2)\n",
    "            base -= af\n",
    "        if pd.notna(rv) and rv>0:\n",
    "            rvf = min(0.5, np.log1p(rv)/9)\n",
    "            base += rvf\n",
    "        if row.get('genres_has_indie',0):\n",
    "            base -=0.25\n",
    "        if row.get('genres_has_casual',0):\n",
    "            base -=0.3\n",
    "        if row.get('genres_has_rpg',0):\n",
    "            base +=0.25\n",
    "        if row.get('genres_has_strategy',0):\n",
    "            base +=0.15\n",
    "        if row.get('is_action_rpg',0):\n",
    "            base +=0.1\n",
    "\n",
    "        return max(-3.0, min(-0.2, base))\n",
    "\n",
    "    xtesteng['price_elasticity'] = xtesteng.apply(est_elasticity, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(xtesteng['price_elasticity'], kde=True, bins=30)\n",
    "    plt.title(\"Price Elasticity Distribution\")\n",
    "    plt.xlabel(\"Price Elasticity\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig('elasticity_distribution_leak_free.png')\n",
    "    plt.close()\n",
    "    print(\"Wrote elasticity distribution figure\")\n",
    "\n",
    "    if 'publishers_is_aaa' in xtesteng.columns:\n",
    "        plt.figure(figsize=(10,6))\n",
    "        sns.boxplot(x='publishers_is_aaa', y='price_elasticity', data=xtesteng)\n",
    "        plt.title(\"Elasticity by Publisher Type\")\n",
    "        plt.xlabel(\"AAA or Not\")\n",
    "        plt.ylabel(\"Elasticity\")\n",
    "        plt.grid(True)\n",
    "        plt.savefig('elasticity_by_publisher_leak_free.png')\n",
    "        plt.close()\n",
    "        print(\"Wrote elasticity by Publisher figure\")\n",
    "\n",
    "    print(\"Making discount recs\")\n",
    "    def rec_discount(row, leak_free=True):\n",
    "        disc = 0.0\n",
    "        ag = row.get('age_years',1)\n",
    "        sc = row.get('combined_score',0.5)\n",
    "        el = row.get('price_elasticity',-1.5)\n",
    "        rv = row.get('reviews_total',0)\n",
    "        ql = row.get('quality_indicator',0.5)\n",
    "        aaa = row.get('publishers_is_aaa',0)\n",
    "\n",
    "        if pd.notna(ag) and ag>0:\n",
    "            if ag<0.5:\n",
    "                aged = 0.05*(ag/0.5)\n",
    "            elif ag<1:\n",
    "                aged = 0.05 + (ag-0.5)*0.15\n",
    "            elif ag<2:\n",
    "                aged = 0.125 + (ag-1)*0.125\n",
    "            else:\n",
    "                aged = 0.25 + min(0.3, (ag-2)*0.1)\n",
    "            disc+=aged\n",
    "        if pd.notna(sc):\n",
    "            if sc<0.4:\n",
    "                scd=0.15\n",
    "            elif sc<0.7:\n",
    "                scd = 0.15 - (sc-0.4)*0.33\n",
    "            else:\n",
    "                scd = 0.05 - (sc-0.7)*0.15\n",
    "            if scd>0:\n",
    "                disc+=scd\n",
    "        if pd.notna(ql):\n",
    "            disc+= (1-ql)*0.15\n",
    "        if pd.notna(el):\n",
    "            nel = np.clip((abs(el)-0.2)/2.8,0,1)\n",
    "            disc+= nel*0.25\n",
    "        if pd.notna(rv) and rv>=0:\n",
    "            lr = np.log1p(rv)\n",
    "            popd = max(0, 0.2*(1-np.clip(lr/8,0,1)))\n",
    "            disc+=popd\n",
    "        if aaa:\n",
    "            if ag<1:\n",
    "                disc-=0.1\n",
    "            elif ag>3:\n",
    "                disc+=0.05\n",
    "        if row.get('genres_has_indie',0):\n",
    "            disc+=0.05\n",
    "        if row.get('genres_has_casual',0):\n",
    "            disc+=0.05\n",
    "        if row.get('genres_has_rpg',0) and not row.get('is_action_rpg',0):\n",
    "            disc-=0.05\n",
    "        if row.get('is_action_rpg',0):\n",
    "            disc-=0.03\n",
    "        if row.get('genres_has_simulation',0):\n",
    "            disc-=0.05\n",
    "\n",
    "        return min(90, max(0, disc*100))\n",
    "\n",
    "    xtesteng['recommended_discount'] = xtesteng.apply(rec_discount, axis=1)\n",
    "\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.histplot(xtesteng['recommended_discount'], bins=30, kde=True)\n",
    "    plt.title(\"distribution recommended discount maybe\")\n",
    "    plt.xlabel(\"discount %\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig('discount_distribution_leak_free.png')\n",
    "    plt.close()\n",
    "    print(\"saved discount distribution figure\")\n",
    "\n",
    "    print(\"Price optimization with best model predictions\")\n",
    "    if best_model_name in final_ridge or best_model_name in final_gb:\n",
    "        moinfo = final_ridge.get(best_model_name) or final_gb.get(best_model_name)\n",
    "        mty = \"Ridge\" if \"Ridge\" in best_model_name else \"GBDT\"\n",
    "        basepred = predict_with_model(xtesteng, moinfo, mty)\n",
    "\n",
    "        def sim_price_change(dff, basep, minf, mtype, pads=None):\n",
    "            if pads is None:\n",
    "                pads = [-30,-20,-10,-5,0,5,10,20,30]\n",
    "            elas = dff['price_elasticity'].values\n",
    "            baseq = np.full(len(basep),100)\n",
    "            baser = basep*baseq\n",
    "            res = []\n",
    "            for ap in pads:\n",
    "                newp = basep*(1+ap/100)\n",
    "                pch = ap/100\n",
    "                qch=[]\n",
    "                for i,e in enumerate(elas):\n",
    "                    if pch>=0:\n",
    "                        dd = pch*e*(1+0.5*pch)\n",
    "                    else:\n",
    "                        dd = pch*e*(1-0.3*pch)\n",
    "                    qch.append(dd)\n",
    "                newq = baseq*(1+np.array(qch))\n",
    "                newq = np.maximum(0,newq)\n",
    "                newr = newp*newq\n",
    "                revc = (newr/baser -1)*100\n",
    "                res.append({\n",
    "                    'price_adj_pct':ap,\n",
    "                    'avg_price':np.mean(newp),\n",
    "                    'avg_quantity':np.mean(newq),\n",
    "                    'avg_revenue':np.mean(newr),\n",
    "                    'avg_rev_change_pct':np.mean(revc)\n",
    "                })\n",
    "            return pd.DataFrame(res)\n",
    "\n",
    "        scen = [-30,-20,-10,-5,0,5,10,15,20,25,30]\n",
    "        simres = sim_price_change(xtesteng, basepred, moinfo, mty, pads=scen)\n",
    "        print(\"sim results for price changes\")\n",
    "        print(simres[['price_adj_pct','avg_price','avg_revenue','avg_rev_change_pct']])\n",
    "\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.plot(simres['price_adj_pct'], simres['avg_rev_change_pct'], 'o-')\n",
    "        plt.axhline(y=0, color='r', linestyle='--')\n",
    "        plt.axvline(x=0, color='r', linestyle='--')\n",
    "        plt.grid(True)\n",
    "        plt.title(\"price change vs revenue change simulation\")\n",
    "        plt.xlabel(\"price change%\")\n",
    "        plt.ylabel(\"rev change%\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('price_elasticity_simulation.png')\n",
    "        plt.close()\n",
    "        print(\"saved simulation result figure\")\n",
    "\n",
    "        bestidx = simres['avg_rev_change_pct'].idxmax()\n",
    "        bestrow = simres.loc[bestidx]\n",
    "        print(\"optimal strategy maybe is= \", bestrow['price_adj_pct'], \" % price adjustment\")\n",
    "        print(\"estimated revenue impact= \", bestrow['avg_rev_change_pct'], \" %\")\n",
    "\n",
    "        print(\"some sample game recs\")\n",
    "        ns = min(5, len(xtesteng))\n",
    "        sids = np.random.choice(len(xtesteng), ns, replace=False)\n",
    "        sFeatures = xtesteng.iloc[sids]\n",
    "        spreds = basepred[sids]\n",
    "        sels = xtesteng.iloc[sids]['price_elasticity'].values\n",
    "        sdisc = xtesteng.iloc[sids]['recommended_discount'].values\n",
    "\n",
    "        stitles=[]\n",
    "        if 'title' in alldata.columns:\n",
    "            stitles = alldata.iloc[ytestreg.iloc[sids].index]['title'].values\n",
    "        else:\n",
    "            stitles = [\"Game \" + str(i+1) for i in range(ns)]\n",
    "        for i,(tt,pp,ee,dd) in enumerate(zip(stitles, spreds, sels, sdisc)):\n",
    "            shortt = (tt[:37]+'...') if len(tt)>40 else tt\n",
    "            print(\"\\nGame= \", shortt)\n",
    "            print(\"pred price= \", round(pp,2), \" elasticity= \", round(ee,2), \" recommended discount= \", round(dd,1), \"%\")\n",
    "            if ee>-1.0:\n",
    "                padj = \"+5% to +15%\"\n",
    "                desc = \"Premium pricing recommended\"\n",
    "            elif ee>-1.5:\n",
    "                padj = \"-5% to +5%\"\n",
    "                desc = \"Maybe keep current price\"\n",
    "            else:\n",
    "                padj = \"-15% to -5%\"\n",
    "                desc = \"Try lowering price\"\n",
    "            print(\"Price adjustment recommendation= \", padj)\n",
    "            print(desc)\n",
    "            newsale = pp*(1-dd/100)\n",
    "            print(\"Sale price around= \", round(newsale,2))\n",
    "    else:\n",
    "        print(\"No best model for price optimization\")\n",
    "\n",
    "    print(\"10) Conclusion\")\n",
    "    print(\"===========================================\")\n",
    "\n",
    "    print(\"comparing final model performance:\")\n",
    "    def format_res(rd):\n",
    "        out={}\n",
    "        for mod, ds in rd.items():\n",
    "            if 'test' in ds:\n",
    "                mm = ds['test']\n",
    "                rr={}\n",
    "                if 'RMSE' in mm:\n",
    "                    rr['RMSE'] = np.mean(mm['RMSE'])\n",
    "                if 'RMSE (orig)' in mm:\n",
    "                    rr['RMSE (orig)'] = np.mean(mm['RMSE (orig)'])\n",
    "                if 'MAE' in mm:\n",
    "                    rr['MAE'] = np.mean(mm['MAE'])\n",
    "                if 'MAPE (%)' in mm:\n",
    "                    rr['MAPE (%)'] = np.mean(mm['MAPE (%)'])\n",
    "                if 'R2' in mm:\n",
    "                    rr['R2'] = np.mean(mm['R2'])\n",
    "                if 'Price Bucket Accuracy' in mm:\n",
    "                    rr['Bucket Acc']= np.mean(mm['Price Bucket Accuracy'])\n",
    "                if 'Price Bucket Accuracy (±1)' in mm:\n",
    "                    rr['Bucket Acc (±1)']= np.mean(mm['Price Bucket Accuracy (±1)'])\n",
    "                out[mod]=rr\n",
    "        return pd.DataFrame(out).T\n",
    "\n",
    "    regmods = {k:v for k,v in allresults.items() if 'LogPrice' not in k}\n",
    "    lgmods = {k:v for k,v in allresults.items() if 'LogPrice' in k}\n",
    "\n",
    "    if regmods:\n",
    "        regdf = format_res(regmods)\n",
    "        if not regdf.empty:\n",
    "            print(\"direct price models:\")\n",
    "            if 'MAE' in regdf.columns:\n",
    "                regdf=regdf.sort_values('MAE')\n",
    "            print(regdf)\n",
    "    if lgmods:\n",
    "        lgdf = format_res(lgmods)\n",
    "        if not lgdf.empty:\n",
    "            print(\"log price models:\")\n",
    "            if 'MAE' in lgdf.columns:\n",
    "                lgdf=lgdf.sort_values('MAE')\n",
    "            elif 'RMSE (orig)' in lgdf.columns:\n",
    "                lgdf=lgdf.sort_values('RMSE (orig)')\n",
    "            print(lgdf)\n",
    "\n",
    "    best_info = None\n",
    "    if best_model_name in final_ridge:\n",
    "        best_info = final_ridge[best_model_name]\n",
    "    elif best_model_name in final_gb:\n",
    "        best_info = final_gb[best_model_name]\n",
    "\n",
    "    if best_info:\n",
    "        print(\"Best model= \", best_model_name)\n",
    "        print(\"MAE= \", best_info['metrics']['MAE'])\n",
    "        if 'Price Bucket Accuracy (±1)' in best_info['metrics']:\n",
    "            pacc = best_info['metrics']['Price Bucket Accuracy (±1)']\n",
    "            print(\"price bracket accuracy ±1= \", pacc)\n",
    "        if best_info['is_log_model'] and 'RMSE (orig)' in best_info['metrics']:\n",
    "            print(\"RMSE on original scale= \", best_info['metrics']['RMSE (orig)'])\n",
    "\n",
    "    print(\"done\")\n",
    "else:\n",
    "    print(\"empty data\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
